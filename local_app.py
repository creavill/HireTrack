#!/usr/bin/env python3
"""
Hammy the Hire Tracker - Local Application
AI-powered job tracking system with Gmail integration and Chrome extension support.

Go HAM on your job search! ðŸ·

This application:
- Scans Gmail for job alerts from LinkedIn, Indeed, and other job boards
- Uses Claude AI to analyze job fit against your resume(s)
- Provides a web dashboard for tracking applications
- Integrates with a Chrome extension for instant job analysis
- Generates tailored cover letters and interview prep answers
- Scans for follow-up emails (interviews, rejections, offers)

Configuration is loaded from config.yaml for easy personalization.
"""

import os
import json
import re
import base64
import sqlite3
import hashlib
import uuid
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, List, Tuple
import urllib.request
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Configuration loader for user preferences
from config_loader import get_config

# Flask web framework for dashboard UI
from flask import Flask, render_template_string, jsonify, request
from flask_cors import CORS
from werkzeug.utils import secure_filename

# Google Gmail API for email scanning
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build

# HTML parsing for job email extraction
from bs4 import BeautifulSoup

# Anthropic Claude AI for job analysis
import anthropic

# Database backup manager
from backup_manager import BackupManager, backup_on_startup

# ============== Configuration ==============
# Load user configuration from config.yaml
try:
    CONFIG = get_config()
except FileNotFoundError as e:
    print(f"\nâŒ Configuration Error: {e}")
    print("ðŸ“ Copy config.example.yaml to config.yaml and fill in your information.\n")
    exit(1)

# Application directories
APP_DIR = Path(__file__).parent
DB_PATH = APP_DIR / "jobs.db"
RESUMES_DIR = APP_DIR / "resumes"
CREDENTIALS_FILE = APP_DIR / "credentials.json"
TOKEN_FILE = APP_DIR / "token.json"

# Gmail API scope for read-only access
SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']

# WeWorkRemotely RSS Feeds
WWR_FEEDS = [
    'https://weworkremotely.com/categories/remote-programming-jobs.rss',
    'https://weworkremotely.com/categories/remote-devops-sysadmin-jobs.rss',
    'https://weworkremotely.com/categories/remote-full-stack-programming-jobs.rss',
]

app = Flask(__name__, static_folder='dist/assets', static_url_path='/assets')
CORS(app)

# ============== Text Cleaning ==============
def clean_text_field(text):
    """
    Thoroughly clean a text field to remove newlines, extra whitespace, and normalize.

    Args:
        text: Input text string

    Returns:
        Cleaned text string
    """
    if not text:
        return ""
    # Replace all newlines and tabs with spaces
    text = text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    # Normalize multiple spaces to single space
    text = ' '.join(text.split())
    # Strip leading/trailing whitespace
    return text.strip()

# ============== URL Cleaning ==============
def clean_job_url(url: str) -> str:
    """
    Remove tracking parameters from job URLs to prevent duplicate entries.

    Job boards add tracking parameters (tracking IDs, referral codes, etc.) that make
    the same job appear as different URLs. This function normalizes URLs by:
    - Extracting core job IDs from LinkedIn and Indeed
    - Removing common tracking parameters (utm_*, refId, etc.)
    - Preserving only essential query parameters

    Args:
        url: Raw job URL with potential tracking parameters

    Returns:
        Cleaned URL with tracking parameters removed

    Examples:
        LinkedIn: https://linkedin.com/jobs/view/123?refId=xyz&trk=email
                  â†’ https://linkedin.com/jobs/view/123
        Indeed:   https://indeed.com/viewjob?jk=abc123&tk=xyz&from=email
                  â†’ https://indeed.com/viewjob?jk=abc123
    """
    if not url:
        return url
    
    parsed = urlparse(url)
    
    # LinkedIn: keep only essential params
    if 'linkedin.com' in parsed.netloc:
        # Extract job ID from path or params
        if '/jobs/view/' in parsed.path:
            job_id = parsed.path.split('/jobs/view/')[-1].split('?')[0].split('/')[0]
            return f"https://www.linkedin.com/jobs/view/{job_id}"
        elif 'currentJobId=' in parsed.query:
            params = parse_qs(parsed.query)
            job_id = params.get('currentJobId', [''])[0]
            if job_id:
                return f"https://www.linkedin.com/jobs/view/{job_id}"
    
    # Indeed: keep only jk param
    elif 'indeed.com' in parsed.netloc:
        params = parse_qs(parsed.query)
        if 'jk' in params:
            return f"https://www.indeed.com/viewjob?jk={params['jk'][0]}"
        elif 'vjk' in params:
            return f"https://www.indeed.com/viewjob?jk={params['vjk'][0]}"
    
    # Remove common tracking params
    if parsed.query:
        params = parse_qs(parsed.query)
        tracking_params = [
            'trackingId', 'refId', 'lipi', 'midToken', 'midSig', 'trk', 
            'trkEmail', 'eid', 'otpToken', 'utm_source', 'utm_medium', 
            'utm_campaign', 'ref', 'source'
        ]
        cleaned_params = {k: v for k, v in params.items() if k not in tracking_params}
        
        if cleaned_params:
            new_query = urlencode(cleaned_params, doseq=True)
            return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', new_query, ''))
        else:
            return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
    
    return url

# ============== Database ==============
def init_db():
    """
    Initialize SQLite database with required tables.

    Creates tables for:
    - jobs: Main job listings with AI analysis and scoring
    - scan_history: Track email scan timestamps to avoid re-processing
    - watchlist: Companies to monitor for future openings
    - followups: Interview and application follow-up tracking
    - external_applications: Track applications made outside the system
    - tracked_companies: Companies to track with career page and job alert email

    Uses WAL (Write-Ahead Logging) mode for better concurrency when
    multiple processes/threads access the database.
    """
    conn = sqlite3.connect(DB_PATH, timeout=30.0)

    # Enable WAL mode for better concurrency (allows simultaneous reads during writes)
    conn.execute("PRAGMA journal_mode=WAL")
    
    # Create tables if they don't exist
    conn.execute('''
        CREATE TABLE IF NOT EXISTS jobs (
            job_id TEXT PRIMARY KEY,
            title TEXT,
            company TEXT,
            location TEXT,
            url TEXT,
            source TEXT,
            status TEXT DEFAULT 'new',
            score INTEGER DEFAULT 0,
            baseline_score INTEGER DEFAULT 0,
            analysis TEXT,
            cover_letter TEXT,
            notes TEXT,
            raw_text TEXT,
            created_at TEXT,
            updated_at TEXT,
            email_date TEXT,
            is_filtered INTEGER DEFAULT 0,
            viewed INTEGER DEFAULT 0
        )
    ''')
    conn.execute('''
        CREATE TABLE IF NOT EXISTS scan_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            last_scan_date TEXT,
            emails_found INTEGER,
            created_at TEXT
        )
    ''')
    conn.execute('''
        CREATE TABLE IF NOT EXISTS watchlist (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            company TEXT NOT NULL,
            url TEXT,
            notes TEXT,
            created_at TEXT
        )
    ''')
    conn.execute('''
        CREATE TABLE IF NOT EXISTS followups (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            company TEXT,
            subject TEXT,
            type TEXT,
            snippet TEXT,
            email_date TEXT,
            job_id TEXT,
            created_at TEXT
        )
    ''')
    conn.execute('''
        CREATE TABLE IF NOT EXISTS external_applications (
            app_id TEXT PRIMARY KEY,
            job_id TEXT,
            title TEXT NOT NULL,
            company TEXT NOT NULL,
            location TEXT,
            url TEXT,
            source TEXT NOT NULL,
            application_method TEXT,
            applied_date TEXT NOT NULL,
            contact_name TEXT,
            contact_email TEXT,
            status TEXT DEFAULT 'applied',
            follow_up_date TEXT,
            notes TEXT,
            created_at TEXT,
            updated_at TEXT,
            is_linked_to_job INTEGER DEFAULT 0,
            FOREIGN KEY (job_id) REFERENCES jobs(job_id)
        )
    ''')

    # Resume management tables
    conn.execute('''
        CREATE TABLE IF NOT EXISTS resume_variants (
            resume_id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            focus_areas TEXT,
            target_roles TEXT,
            file_path TEXT,
            content TEXT,
            content_hash TEXT,
            usage_count INTEGER DEFAULT 0,
            created_at TEXT,
            updated_at TEXT,
            is_active INTEGER DEFAULT 1
        )
    ''')

    conn.execute('''
        CREATE TABLE IF NOT EXISTS resume_usage_log (
            log_id TEXT PRIMARY KEY,
            resume_id TEXT,
            job_id TEXT,
            recommended_at TEXT,
            confidence_score REAL,
            user_selected INTEGER DEFAULT 0,
            reasoning TEXT,
            FOREIGN KEY (resume_id) REFERENCES resume_variants(resume_id),
            FOREIGN KEY (job_id) REFERENCES jobs(job_id)
        )
    ''')

    conn.execute('''
        CREATE TABLE IF NOT EXISTS tracked_companies (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            company_name TEXT NOT NULL,
            career_page_url TEXT,
            job_alert_email TEXT,
            notes TEXT,
            created_at TEXT,
            updated_at TEXT
        )
    ''')

    conn.execute('''
        CREATE TABLE IF NOT EXISTS custom_email_sources (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            sender_email TEXT,
            sender_pattern TEXT,
            subject_keywords TEXT,
            enabled INTEGER DEFAULT 1,
            created_at TEXT,
            updated_at TEXT
        )
    ''')

    # Migration: Add job_id column if it doesn't exist
    try:
        conn.execute("SELECT job_id FROM followups LIMIT 1")
    except sqlite3.OperationalError:
        print("Migrating database: adding 'job_id' column to followups...")
        conn.execute("ALTER TABLE followups ADD COLUMN job_id TEXT")
    
    # Migration: Add viewed column if it doesn't exist
    try:
        conn.execute("SELECT viewed FROM jobs LIMIT 1")
    except sqlite3.OperationalError:
        print("Migrating database: adding 'viewed' column...")
        conn.execute("ALTER TABLE jobs ADD COLUMN viewed INTEGER DEFAULT 0")

    # Migration: Add resume-related columns to jobs table
    try:
        conn.execute("SELECT recommended_resume_id FROM jobs LIMIT 1")
    except sqlite3.OperationalError:
        print("Migrating database: adding resume-related columns to jobs...")
        conn.execute("ALTER TABLE jobs ADD COLUMN recommended_resume_id TEXT")
        conn.execute("ALTER TABLE jobs ADD COLUMN resume_recommendation TEXT")
        conn.execute("ALTER TABLE jobs ADD COLUMN selected_resume_id TEXT")
        conn.execute("ALTER TABLE jobs ADD COLUMN resume_match_score REAL")

    conn.commit()
    conn.close()

def get_db():
    conn = sqlite3.connect(DB_PATH, timeout=30.0)  # Wait up to 30s for lock
    conn.row_factory = sqlite3.Row
    return conn

# ============== Gmail ==============
def get_gmail_service():
    creds = None
    if TOKEN_FILE.exists():
        creds = Credentials.from_authorized_user_file(str(TOKEN_FILE), SCOPES)
    
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            if not CREDENTIALS_FILE.exists():
                raise FileNotFoundError(
                    f"Missing {CREDENTIALS_FILE}. Download from Google Cloud Console."
                )
            flow = InstalledAppFlow.from_client_secrets_file(str(CREDENTIALS_FILE), SCOPES)
            creds = flow.run_local_server(port=0)
        
        with open(TOKEN_FILE, 'w') as f:
            f.write(creds.to_json())
    
    return build('gmail', 'v1', credentials=creds)

def get_email_body(payload):
    body = ""
    if 'body' in payload and payload['body'].get('data'):
        body = base64.urlsafe_b64decode(payload['body']['data']).decode('utf-8')
    elif 'parts' in payload:
        for part in payload['parts']:
            if part['mimeType'] == 'text/html' and 'data' in part.get('body', {}):
                body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')
                break
            elif 'parts' in part:
                body = get_email_body(part)
                if body:
                    break
    return body

def generate_job_id(url, title, company):
    # Use cleaned URL for consistent ID generation
    clean_url = clean_job_url(url)
    content = f"{clean_url}:{title}:{company}".lower()
    return hashlib.sha256(content.encode()).hexdigest()[:16]

def parse_linkedin_jobs(html, email_date):
    """Extract LinkedIn jobs with better filtering."""
    jobs = []
    soup = BeautifulSoup(html, 'html.parser')
    
    # Find job links with actual job IDs
    job_links = soup.find_all('a', href=re.compile(r'linkedin\.com.*jobs/view/\d{10}'))
    
    exclude_keywords = ['see all', 'unsubscribe', 'help', 'saved jobs', 'search jobs', 
                       'learn why', 'settings', 'preferences', 'view all', 'messaging', 
                       'mynetwork', 'games', 'notifications']
    
    seen = set()
    for link in job_links:
        url = clean_job_url(link.get('href', ''))
        if not url or url in seen:
            continue
        
        # Get title from link text or nearby heading
        title_elem = link.find(['h3', 'h4', 'span', 'div'])
        full_text = title_elem.get_text(separator=' ', strip=True) if title_elem else link.get_text(separator=' ', strip=True)
        # Clean up newlines and extra whitespace
        full_text = ' '.join(full_text.split())

        # Skip if title is a UI element
        if any(keyword in full_text.lower() for keyword in exclude_keywords):
            continue
        
        if not full_text or len(full_text) < 5:
            continue
        
        seen.add(url)
        
        # Parse formats:
        # "Software EngineerLensa Â· San Diego, CA"
        # "DevOps EngineerHumana Â· United States (Remote)"
        title = full_text
        company = ""
        location = ""
        
        # Split on delimiter
        if 'Â·' in full_text:
            parts = full_text.split('Â·', 1)
            title_company_part = parts[0].strip()
            location = parts[1].strip() if len(parts) > 1 else ""
            
            # Company is capitalized word(s) after lowercase letter
            # Match patterns like "EngineerCompanyName" or "DeveloperLensa"
            match = re.search(r'([a-z])([A-Z][A-Za-z0-9\s&.,-]+)$', title_company_part)
            if match:
                title = title_company_part[:match.start(2)].strip()
                company = match.group(2).strip()
            else:
                # Fallback: split on last capital letter sequence
                capital_match = re.search(r'^(.+?)([A-Z][A-Za-z0-9\s&.,-]+)$', title_company_part)
                if capital_match:
                    title = capital_match.group(1).strip()
                    company = capital_match.group(2).strip()
                else:
                    title = title_company_part
        
        # Fallback: check parent for more context
        if not company:
            parent = link.find_parent(['div', 'td', 'tr', 'li'])
            if parent:
                text = parent.get_text('\n', strip=True)
                lines = [l.strip() for l in text.split('\n') if l.strip()]
                # Look for company name in next line
                for i, line in enumerate(lines):
                    if title in line and i + 1 < len(lines):
                        next_line = lines[i + 1]
                        # Company usually doesn't have symbols except &
                        if not any(c in next_line for c in ['$', '/', '(', ')', 'Easy Apply', 'Actively']):
                            company = next_line.split('Â·')[0].strip()[:100]
                        break
        
        raw_text = full_text
        parent = link.find_parent(['div', 'td', 'tr', 'li'])
        if parent:
            text = parent.get_text('\n', strip=True)
            lines = [l.strip() for l in text.split('\n') if l.strip()]
            raw_text = ' '.join(lines[:5])[:1000]
        
        # Final cleanup of all text fields
        title = clean_text_field(title)
        company = clean_text_field(company) if company else "Unknown"
        location = clean_text_field(location)

        jobs.append({
            'job_id': generate_job_id(url, title, company),
            'title': title[:200],
            'company': company[:100],
            'location': location[:100],
            'url': url,
            'source': 'linkedin',
            'raw_text': raw_text,
            'created_at': email_date,
            'email_date': email_date
        })
    
    return jobs


def parse_indeed_jobs(html, email_date):
    """Extract Indeed jobs with better filtering."""
    jobs = []
    soup = BeautifulSoup(html, 'html.parser')
    
    # Indeed uses table cells or divs for job cards
    job_links = soup.find_all('a', href=re.compile(r'indeed\.com.*(jk=|vjk=)[a-f0-9]+'))
    
    exclude_keywords = ['unsubscribe', 'help', 'view all', 'see all', 'homepage', 
                       'messages', 'notifications', 'easily apply', 'responsive employer']
    
    seen = set()
    for link in job_links:
        url = clean_job_url(link.get('href', ''))
        if not url or url in seen:
            continue

        full_text = link.get_text(separator=' ', strip=True)
        # Clean up newlines and extra whitespace
        full_text = ' '.join(full_text.split())

        # Skip UI elements
        if any(keyword in full_text.lower() for keyword in exclude_keywords):
            continue
        
        if not full_text or len(full_text) < 5:
            continue
        
        seen.add(url)
        
        title = full_text
        company = ""
        location = ""
        
        # Find parent container
        parent = link.find_parent(['td', 'div', 'li'])
        raw_text = full_text
        
        if parent:
            text = parent.get_text('\n', strip=True)
            lines = [l.strip() for l in text.split('\n') if l.strip() and len(l.strip()) > 2]
            
            # Indeed format: Title / Company / Location / Salary / Description
            for i, line in enumerate(lines):
                if title in line and i + 1 < len(lines):
                    # Next line is usually company
                    potential_company = lines[i + 1]
                    # Skip ratings like "4.8 4.8/5 rating"
                    if not re.match(r'^\d+\.?\d*\s*\d', potential_company):
                        company = potential_company[:100]
                    if i + 2 < len(lines):
                        potential_location = lines[i + 2]
                        if 'remote' in potential_location.lower() or ',' in potential_location:
                            location = potential_location[:100]
                    break
            
            raw_text = ' '.join(lines[:6])[:1000]

        # Final cleanup of all text fields
        title = clean_text_field(title)
        company = clean_text_field(company) if company else "Unknown"
        location = clean_text_field(location)

        jobs.append({
            'job_id': generate_job_id(url, title, company),
            'title': title[:200],
            'company': company[:100],
            'location': location[:100],
            'url': url,
            'source': 'indeed',
            'raw_text': raw_text,
            'created_at': email_date,
            'email_date': email_date
        })
    
    return jobs


def parse_greenhouse_jobs(html, email_date):
    """Extract Greenhouse jobs."""
    jobs = []
    soup = BeautifulSoup(html, 'html.parser')
    
    # Greenhouse links
    job_links = soup.find_all('a', href=re.compile(r'greenhouse\.io|boards\.greenhouse\.io'))
    
    seen = set()
    for link in job_links:
        url = link.get('href', '')
        if not url or url in seen or 'unsubscribe' in url.lower():
            continue
        
        url = clean_job_url(url)

        # Title from link or nearby
        title = link.get_text(separator=' ', strip=True)
        # Clean up newlines and extra whitespace
        title = ' '.join(title.split())
        if not title or len(title) < 5:
            continue
        
        seen.add(url)
        
        # Find company and location
        parent = link.find_parent(['div', 'td', 'tr'])
        company, location, raw_text = "", "", title
        
        if parent:
            text = parent.get_text('\n', strip=True)
            lines = [l.strip() for l in text.split('\n') if l.strip()]
            
            # Extract company from URL or nearby text
            if 'boards.greenhouse.io' in url:
                company = url.split('boards.greenhouse.io/')[-1].split('/')[0].replace('-', ' ').title()
            
            for line in lines:
                if 'engineering' in line.lower() or 'department' in line.lower():
                    continue
                if any(loc in line.lower() for loc in ['remote', 'hybrid', 'san diego', 'california']):
                    location = line[:100]
                    break
            
            raw_text = ' '.join(lines[:5])[:1000]

        # Final cleanup of all text fields
        title = clean_text_field(title)
        company = clean_text_field(company) if company else "Unknown"
        location = clean_text_field(location)

        jobs.append({
            'job_id': generate_job_id(url, title, company),
            'title': title[:200],
            'company': company[:100],
            'location': location[:100],
            'url': url,
            'source': 'greenhouse',
            'raw_text': raw_text,
            'created_at': email_date,
            'email_date': email_date
        })
    
    return jobs


def parse_wellfound_jobs(html, email_date):
    """Extract Wellfound (AngelList) jobs."""
    jobs = []
    soup = BeautifulSoup(html, 'html.parser')
    
    # Wellfound links
    job_links = soup.find_all('a', href=re.compile(r'wellfound\.com|angel\.co'))
    
    exclude_keywords = ['unsubscribe', 'settings', 'preferences', 'learn more']
    
    seen = set()
    for link in job_links:
        url = link.get('href', '')
        if not url or url in seen:
            continue
        
        url = clean_job_url(url)
        title = link.get_text(separator=' ', strip=True)
        # Clean up newlines and extra whitespace
        title = ' '.join(title.split())

        if any(keyword in title.lower() for keyword in exclude_keywords):
            continue
        
        if not title or len(title) < 5:
            continue
        
        seen.add(url)
        
        # Find company and details
        parent = link.find_parent(['div', 'td', 'tr'])
        company, location, raw_text = "", "Remote", title
        
        if parent:
            text = parent.get_text('\n', strip=True)
            lines = [l.strip() for l in text.split('\n') if l.strip()]
            
            # Wellfound format includes company info nearby
            for i, line in enumerate(lines):
                if '/' in line and 'Employees' in line:
                    company = line.split('/')[0].strip()[:100]
                if any(loc in line for loc in ['Remote', 'Austin', 'San Diego', 'San Francisco']):
                    location = line[:100]
            
            raw_text = ' '.join(lines[:8])[:1000]

        # Final cleanup of all text fields
        title = clean_text_field(title)
        company = clean_text_field(company) if company else "Unknown"
        location = clean_text_field(location)

        jobs.append({
            'job_id': generate_job_id(url, title, company),
            'title': title[:200],
            'company': company[:100],
            'location': location[:100],
            'url': url,
            'source': 'wellfound',
            'raw_text': raw_text,
            'created_at': email_date,
            'email_date': email_date
        })
    
    return jobs

def fetch_wwr_jobs(days_back=7):
    """Fetch jobs from WeWorkRemotely RSS feeds."""
    jobs = []
    cutoff = datetime.now() - timedelta(days=days_back)
    
    for feed_url in WWR_FEEDS:
        try:
            req = urllib.request.Request(feed_url, headers={'User-Agent': 'JobTracker/1.0'})
            with urllib.request.urlopen(req, timeout=10) as response:
                xml_data = response.read()
            
            root = ET.fromstring(xml_data)
            
            for item in root.findall('.//item'):
                title_elem = item.find('title')
                link_elem = item.find('link')
                desc_elem = item.find('description')
                pub_date_elem = item.find('pubDate')
                
                if title_elem is None or link_elem is None:
                    continue
                
                title = title_elem.text or ''
                url = clean_job_url(link_elem.text or '')
                description = desc_elem.text if desc_elem is not None else ''
                
                company = ''
                job_title = title
                if ':' in title:
                    parts = title.split(':', 1)
                    company = parts[0].strip()
                    job_title = parts[1].strip()
                
                pub_date = datetime.now().isoformat()
                if pub_date_elem is not None and pub_date_elem.text:
                    try:
                        from email.utils import parsedate_to_datetime
                        dt = parsedate_to_datetime(pub_date_elem.text)
                        if dt < cutoff:
                            continue
                        pub_date = dt.isoformat()
                    except:
                        pass
                
                if description:
                    soup = BeautifulSoup(description, 'html.parser')
                    description = soup.get_text(' ', strip=True)[:2000]
                
                job_id = generate_job_id(url, job_title, company)
                
                jobs.append({
                    'job_id': job_id,
                    'title': job_title[:200],
                    'company': company[:100],
                    'location': 'Remote',
                    'url': url,
                    'source': 'weworkremotely',
                    'raw_text': description or title,
                    'description': description,
                    'created_at': pub_date,
                    'email_date': pub_date
                })
                
        except Exception as e:
            print(f"WWR feed error ({feed_url}): {e}")
    
    return jobs

def scan_emails(days_back=7):
    service = get_gmail_service()
    
    # Get last scan date from DB
    conn = sqlite3.connect(DB_PATH)
    last_scan = conn.execute(
        "SELECT last_scan_date FROM scan_history ORDER BY created_at DESC LIMIT 1"
    ).fetchone()
    conn.close()
    
    if last_scan and last_scan[0]:
        # Parse the saved date and format for Gmail
        try:
            last_date = datetime.fromisoformat(last_scan[0])
            # Add 1 second to avoid re-processing last scan's emails
            last_date = last_date + timedelta(seconds=1)
            after_date = last_date.strftime('%Y/%m/%d')
            print(f"ðŸ“… Scanning emails after last scan: {after_date}")
        except:
            # Fallback if date parsing fails
            after_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y/%m/%d')
            print(f"âš ï¸ Date parse failed, scanning last {days_back} days: {after_date}")
    else:
        after_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y/%m/%d')
        print(f"ðŸ†• First scan - looking back {days_back} days: {after_date}")
    
    # Query all job board emails + follow-ups
    queries = [
        # Known job alerts
        f'from:jobs-noreply@linkedin.com after:{after_date}',
        f'from:jobalerts-noreply@linkedin.com after:{after_date}',
        f'from:noreply@indeed.com after:{after_date}',
        f'from:alert@indeed.com after:{after_date}',
        f'from:no-reply@us.greenhouse-jobs.com after:{after_date}',
        f'from:team@hi.wellfound.com after:{after_date}',

        # Generic job board catch-all (job OR career OR position in subject)
        f'(subject:job OR subject:career OR subject:position OR subject:"now hiring") -from:linkedin.com -from:indeed.com -from:greenhouse.io -from:wellfound.com after:{after_date}',

        # Follow-ups for Applied tab
        f'(subject:interview OR subject:"next steps" OR subject:update OR subject:"application received" OR subject:confirmation) after:{after_date}',
        f'(subject:unfortunately OR subject:offer OR subject:congratulations OR subject:"thank you for applying") after:{after_date}',
    ]

    # Add custom email sources from database
    conn_sources = sqlite3.connect(DB_PATH)
    custom_sources = conn_sources.execute(
        "SELECT name, sender_email, sender_pattern, subject_keywords FROM custom_email_sources WHERE enabled = 1"
    ).fetchall()
    conn_sources.close()

    for source in custom_sources:
        source_name, sender_email, sender_pattern, subject_keywords = source
        query_parts = []

        # Add sender filter
        if sender_email:
            query_parts.append(f'from:{sender_email}')
        elif sender_pattern:
            # Gmail doesn't support wildcards in from, so use the pattern as-is
            query_parts.append(f'from:{sender_pattern}')

        # Add subject keywords if provided
        if subject_keywords:
            keywords = [kw.strip() for kw in subject_keywords.split(',')]
            subject_part = ' OR '.join([f'subject:{kw}' for kw in keywords if kw])
            if subject_part:
                query_parts.append(f'({subject_part})')

        # Combine query parts
        if query_parts:
            custom_query = ' '.join(query_parts) + f' after:{after_date}'
            queries.append(custom_query)
            print(f"ðŸ“§ Added custom source: {source_name} -> {custom_query}")

    
    all_jobs = []
    seen_job_ids = set()  # Track job_ids to prevent duplicates
    total_emails = 0
    
    for query in queries:
        try:
            # Increased from 50 to 100
            results = service.users().messages().list(userId='me', q=query, maxResults=100).execute()
            messages = results.get('messages', [])
            total_emails += len(messages)
            
            for msg_info in messages:
                try:
                    message = service.users().messages().get(userId='me', id=msg_info['id'], format='full').execute()
                    email_date = datetime.fromtimestamp(int(message.get('internalDate', 0)) / 1000).isoformat()
                    html = get_email_body(message.get('payload', {}))
                    
                    if not html:
                        continue
                    
                    # Check if follow-up email
                    subject = ''
                    for header in message.get('payload', {}).get('headers', []):
                        if header['name'].lower() == 'subject':
                            subject = header['value'].lower()
                            break
                    
                    is_followup = any(keyword in subject for keyword in [
                        'interview', 'next steps', 'unfortunately', 'offer', 
                        'congratulations', 'declined', 'application update'
                    ])
                    
                    if is_followup:
                        print(f"ðŸ“§ Follow-up detected: {subject[:60]}...")
                        # TODO: Add follow-up tracking table and logic
                        continue
                    
                    # Route to appropriate parser
                    if 'linkedin' in query:
                        jobs = parse_linkedin_jobs(html, email_date)
                    elif 'indeed' in query:
                        jobs = parse_indeed_jobs(html, email_date)
                    elif 'greenhouse' in query:
                        jobs = parse_greenhouse_jobs(html, email_date)
                    elif 'wellfound' in query:
                        jobs = parse_wellfound_jobs(html, email_date)
                    else:
                        jobs = []
                    
                    # Deduplicate before adding
                    unique_jobs = []
                    for job in jobs:
                        if job['job_id'] not in seen_job_ids:
                            seen_job_ids.add(job['job_id'])
                            unique_jobs.append(job)
                    
                    all_jobs.extend(unique_jobs)
                    if unique_jobs:
                        print(f"âœ“ Found {len(unique_jobs)} unique jobs from {query.split('from:')[1].split()[0] if 'from:' in query else 'follow-ups'}")
                    
                except Exception as e:
                    print(f"Error parsing email {msg_info['id']}: {e}")
                    continue
                    
        except Exception as e:
            print(f"Query failed - {query}: {e}")
            continue
    
    # Save current scan timestamp (ISO format for consistency)
    current_scan_time = datetime.now().isoformat()
    conn = sqlite3.connect(DB_PATH)
    conn.execute(
        "INSERT INTO scan_history (last_scan_date, emails_found, created_at) VALUES (?, ?, ?)",
        (current_scan_time, total_emails, current_scan_time)
    )
    conn.commit()
    conn.close()
    
    print(f"âœ… Scan complete: {total_emails} emails checked, {len(all_jobs)} unique jobs extracted")
    print(f"ðŸ“Œ Next scan will query emails after: {current_scan_time}")
    return all_jobs

# ============== Follow-Up Email Scanning ==============
def classify_followup_email(subject: str, snippet: str) -> str:
    """
    Classify follow-up email type based on subject and snippet.

    Uses keyword matching to determine if email is:
    - interview: Interview request or scheduling
    - rejection: Application declined
    - received: Application confirmation
    - offer: Job offer
    - assessment: Coding challenge or take-home project

    Args:
        subject: Email subject line (lowercase)
        snippet: Email preview text (lowercase)

    Returns:
        Email type string
    """
    text = (subject + " " + snippet).lower()

    # Check for interview requests (highest priority)
    if any(word in text for word in ['interview', 'phone screen', 'video call', 'meet the team',
                                       'schedule a call', 'next steps', 'speak with', 'chat with']):
        return 'interview'

    # Check for offers
    if any(word in text for word in ['offer', 'congratulations', 'pleased to extend',
                                      'compensation package', 'welcome to the team']):
        return 'offer'

    # Check for assessments/challenges
    if any(word in text for word in ['assessment', 'coding challenge', 'take-home',
                                      'technical exercise', 'complete the', 'test project']):
        return 'assessment'

    # Check for rejections
    if any(word in text for word in ['unfortunately', 'not moving forward', 'other candidates',
                                      'decided to pursue', 'not selected', 'will not be moving',
                                      'unable to move forward', 'chosen to move forward with']):
        return 'rejection'

    # Check for application received confirmations
    if any(word in text for word in ['received your application', 'thank you for applying',
                                      'application has been', 'reviewing your', 'under review']):
        return 'received'

    # Default: general update
    return 'update'


def extract_company_from_email(from_email: str, subject: str) -> str:
    """
    Extract company name from email sender or subject.

    Args:
        from_email: Sender email address
        subject: Email subject line

    Returns:
        Extracted company name or 'Unknown'
    """
    # Try to extract from email domain
    if '@' in from_email:
        domain = from_email.split('@')[1].lower()

        # Remove common suffixes
        company = domain.replace('.com', '').replace('.io', '').replace('.co', '')
        company = company.replace('greenhouse', '').replace('lever', '').replace('workday', '')

        # Skip generic domains
        if company not in ['gmail', 'outlook', 'yahoo', 'hotmail', 'mail', 'email']:
            return company.title()

    # Try to extract from subject (e.g., "Your Application at Company Name")
    patterns = [
        r'at\s+([A-Z][A-Za-z0-9\s&.,-]+)',
        r'with\s+([A-Z][A-Za-z0-9\s&.,-]+)',
        r'from\s+([A-Z][A-Za-z0-9\s&.,-]+)',
    ]

    for pattern in patterns:
        match = re.search(pattern, subject)
        if match:
            company = match.group(1).strip()
            # Remove common trailing words
            company = re.sub(r'\s+(team|recruiting|talent|careers)$', '', company, flags=re.IGNORECASE)
            return company[:50]  # Limit length

    return 'Unknown'


def fuzzy_match_company(email_company: str, conn) -> Optional[str]:
    """
    Find matching job in database by fuzzy company name matching.

    Args:
        email_company: Company name extracted from email
        conn: Database connection

    Returns:
        job_id if match found, None otherwise
    """
    # Try exact match first
    result = conn.execute(
        "SELECT job_id FROM jobs WHERE LOWER(company) = ? AND status IN ('applied', 'interviewing')",
        (email_company.lower(),)
    ).fetchone()

    if result:
        return result[0]

    # Try partial match (company name contains or is contained in email company)
    applied_jobs = conn.execute(
        "SELECT job_id, company FROM jobs WHERE status IN ('applied', 'interviewing')"
    ).fetchall()

    for job in applied_jobs:
        job_id, job_company = job[0], job[1].lower()
        email_comp_lower = email_company.lower()

        # Check if one contains the other
        if job_company in email_comp_lower or email_comp_lower in job_company:
            return job_id

        # Check for common abbreviations (e.g., "Meta" vs "Facebook")
        company_map = {
            'meta': 'facebook',
            'google': 'alphabet',
            'aws': 'amazon',
        }

        for key, value in company_map.items():
            if (key in job_company and value in email_comp_lower) or \
               (value in job_company and key in email_comp_lower):
                return job_id

    return None


def scan_followup_emails(days_back: int = 30) -> List[Dict]:
    """
    Scan Gmail for follow-up emails (interviews, rejections, offers).

    Checks both inbox and spam folders for responses to job applications.
    Automatically classifies emails and matches them to jobs in database.

    Args:
        days_back: How many days back to scan (default: 30)

    Returns:
        List of follow-up email dictionaries
    """
    service = get_gmail_service()
    after_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y/%m/%d')

    # Queries for different types of follow-ups
    queries = [
        # Interview requests
        f'(subject:interview OR subject:"phone screen" OR subject:"next steps" OR subject:"schedule") after:{after_date}',

        # Rejections (check spam too!)
        f'(subject:unfortunately OR subject:"not selected" OR subject:"other candidates" OR subject:"decided to pursue") after:{after_date}',

        # Offers
        f'(subject:offer OR subject:congratulations OR subject:"pleased to extend") after:{after_date}',

        # Assessments
        f'(subject:assessment OR subject:"coding challenge" OR subject:"take-home") after:{after_date}',

        # Application confirmations
        f'(subject:"received your application" OR subject:"thank you for applying") after:{after_date}',
    ]

    followups = []
    seen_message_ids = set()

    # Scan both INBOX and SPAM
    for folder in ['INBOX', '[Gmail]/Spam']:
        for query in queries:
            try:
                # Modify query to search in specific folder
                if folder == '[Gmail]/Spam':
                    search_query = f'in:spam {query}'
                else:
                    search_query = query

                results = service.users().messages().list(userId='me', q=search_query, maxResults=50).execute()
                messages = results.get('messages', [])

                for msg_info in messages:
                    msg_id = msg_info['id']

                    # Skip duplicates
                    if msg_id in seen_message_ids:
                        continue
                    seen_message_ids.add(msg_id)

                    try:
                        message = service.users().messages().get(userId='me', id=msg_id, format='full').execute()

                        # Extract email details
                        headers = message.get('payload', {}).get('headers', [])
                        subject = ''
                        from_email = ''

                        for header in headers:
                            if header['name'].lower() == 'subject':
                                subject = header['value']
                            elif header['name'].lower() == 'from':
                                from_email = header['value']

                        snippet = message.get('snippet', '')
                        email_date = datetime.fromtimestamp(int(message.get('internalDate', 0)) / 1000).isoformat()

                        # Classify email type
                        email_type = classify_followup_email(subject, snippet)

                        # Extract company name
                        company = extract_company_from_email(from_email, subject)

                        # Try to match to a job in database
                        conn = get_db()
                        job_id = fuzzy_match_company(company, conn)
                        conn.close()

                        followups.append({
                            'company': company,
                            'subject': subject[:200],
                            'type': email_type,
                            'snippet': snippet[:500],
                            'email_date': email_date,
                            'job_id': job_id,
                            'in_spam': folder == '[Gmail]/Spam'
                        })

                        print(f"ðŸ“§ {email_type.upper()}: {company} - {subject[:50]}... (spam: {folder == '[Gmail]/Spam'})")

                    except Exception as e:
                        print(f"Error parsing follow-up email {msg_id}: {e}")
                        continue

            except Exception as e:
                print(f"Query failed - {search_query}: {e}")
                continue

    print(f"\nâœ… Follow-up scan complete: {len(followups)} emails found")
    return followups

# ============== AI Filtering & Scoring ==============
def load_resumes() -> str:
    """
    Load all resume files from configured paths.

    Reads resume files specified in config.yaml and concatenates them
    with separators for AI analysis. Supports both .txt and .md formats.

    Returns:
        Combined resume text from all configured files

    Raises:
        FileNotFoundError: If configured resume files don't exist
    """
    resumes = []

    # Load resumes from configured file paths
    for resume_path in CONFIG.resume_files:
        full_path = APP_DIR / resume_path
        if full_path.exists():
            resumes.append(full_path.read_text())
        else:
            print(f"âš ï¸  Warning: Resume file not found: {resume_path}")

    if not resumes:
        raise FileNotFoundError(
            "No resume files found! Add resume files to the resumes/ directory "
            "and configure them in config.yaml"
        )

    return "\n\n---\n\n".join(resumes)


def migrate_file_resumes_to_db():
    """
    One-time migration to import existing resume files into the database.
    Checks if resumes already exist to avoid duplicates.
    """
    conn = get_db()
    migrated = 0

    for resume_path in CONFIG.resume_files:
        full_path = APP_DIR / resume_path
        if not full_path.exists():
            continue

        # Read resume content
        content = full_path.read_text()
        content_hash = hashlib.sha256(content.encode()).hexdigest()

        # Check if already exists
        existing = conn.execute(
            "SELECT resume_id FROM resume_variants WHERE content_hash = ?",
            (content_hash,)
        ).fetchone()

        if existing:
            print(f"âœ“ Resume already in database: {resume_path}")
            continue

        # Create resume entry
        resume_id = str(uuid.uuid4())[:16]
        name = full_path.stem.replace('_', ' ').title()
        now = datetime.now().isoformat()

        conn.execute('''
            INSERT INTO resume_variants (
                resume_id, name, file_path, content, content_hash,
                created_at, updated_at, is_active
            ) VALUES (?, ?, ?, ?, ?, ?, ?, 1)
        ''', (resume_id, name, str(resume_path), content, content_hash, now, now))

        migrated += 1
        print(f"âœ“ Migrated resume: {name}")

    conn.commit()
    conn.close()

    if migrated > 0:
        print(f"\nâœ… Migrated {migrated} resume(s) to database!")
    return migrated


def load_resumes_from_db() -> List[Dict]:
    """
    Load all active resume variants from the database.

    Returns:
        List of resume dictionaries with id, name, content, etc.
    """
    conn = get_db()
    resumes = conn.execute("""
        SELECT * FROM resume_variants
        WHERE is_active = 1
        ORDER BY usage_count DESC, created_at DESC
    """).fetchall()
    conn.close()

    return [dict(r) for r in resumes]


def get_combined_resume_text() -> str:
    """
    Get combined text from all active resumes for AI analysis.
    Falls back to file-based loading if no resumes in database.

    Returns:
        Combined resume text from all active resumes
    """
    resumes = load_resumes_from_db()

    if not resumes:
        # Fallback to file-based loading
        print("âš ï¸  No resumes in database, falling back to file-based loading...")
        return load_resumes()

    return "\n\n---\n\n".join([r['content'] for r in resumes])


def recommend_resume_for_job(job_description: str, job_title: str = "", job_company: str = "") -> Dict:
    """
    Use Claude AI to recommend the best resume for a specific job.

    Args:
        job_description: Full job description text
        job_title: Job title (optional, for context)
        job_company: Company name (optional, for context)

    Returns:
        Dictionary with recommendation details:
        {
            'resume_id': str,
            'resume_name': str,
            'confidence': float (0-1),
            'reasoning': str,
            'key_requirements': List[str],
            'resume_strengths': List[str],
            'resume_gaps': List[str],
            'alternative_resumes': List[Dict]
        }
    """
    resumes = load_resumes_from_db()

    if not resumes:
        raise ValueError("No resumes available. Please upload at least one resume.")

    # Format resumes for AI
    resume_catalog = "\n\n".join([
        f"Resume ID: {r['resume_id']}\n"
        f"Name: {r['name']}\n"
        f"Focus Areas: {r.get('focus_areas', 'Not specified')}\n"
        f"Target Roles: {r.get('target_roles', 'Not specified')}\n"
        f"Content Preview: {r['content'][:500]}..."
        for r in resumes
    ])

    job_context = f"Job: {job_title} at {job_company}\n\n" if job_title else ""

    prompt = f"""You are a resume selection expert. Analyze this job description and recommend
the BEST resume from the available options.

{job_context}Job Description:
{job_description[:2500]}

Available Resumes:
{resume_catalog}

Analyze the job requirements and each resume's strengths. Return a JSON object with your recommendation:

{{
  "recommended_resume_id": "<resume_id>",
  "confidence": 0.85,
  "reasoning": "This role emphasizes Python microservices and AWS Lambda which aligns perfectly with your Backend_Python_AWS resume's core strengths...",
  "key_requirements": ["Python", "AWS Lambda", "REST APIs", "PostgreSQL"],
  "resume_strengths": ["5 years Python experience", "AWS Lambda projects", "FastAPI expertise"],
  "resume_gaps": ["No Kubernetes mentioned"],
  "alternative_resumes": [
    {{
      "resume_id": "<other_resume_id>",
      "resume_name": "Cloud_AWS",
      "confidence": 0.70,
      "reason": "Strong AWS background but less Python depth"
    }}
  ]
}}

Only recommend alternatives if your confidence in the primary recommendation is below 0.9.
Be specific about technical requirements and how the resume matches them."""

    try:
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )

        # Parse JSON response
        response_text = response.content[0].text

        # Extract JSON from response (handle markdown code blocks)
        if "```json" in response_text:
            json_str = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            json_str = response_text.split("```")[1].split("```")[0].strip()
        else:
            json_str = response_text.strip()

        recommendation = json.loads(json_str)

        # Find the recommended resume details
        recommended_resume = next(
            (r for r in resumes if r['resume_id'] == recommendation['recommended_resume_id']),
            resumes[0]  # Fallback to first resume
        )

        # Add resume name to response
        recommendation['resume_name'] = recommended_resume['name']

        return recommendation

    except Exception as e:
        print(f"Error in resume recommendation: {e}")
        # Fallback: return first resume with low confidence
        return {
            'resume_id': resumes[0]['resume_id'],
            'resume_name': resumes[0]['name'],
            'confidence': 0.5,
            'reasoning': f"Unable to generate AI recommendation ({str(e)}). Defaulting to first available resume.",
            'key_requirements': [],
            'resume_strengths': [],
            'resume_gaps': [],
            'alternative_resumes': []
        }


def ai_filter_and_score(job: Dict, resume_text: str) -> Tuple[bool, int, str]:
    """
    AI-based job filtering and baseline scoring using Claude.

    Uses Claude AI to:
    1. Filter jobs by location preferences (from config.yaml)
    2. Filter overly senior/junior roles based on experience level
    3. Generate a baseline score (1-100) based on location, seniority, company, and tech stack

    Jobs that don't match location preferences or are way outside experience level
    are filtered out to reduce noise.

    Args:
        job: Job dictionary with title, company, location, and raw_text
        resume_text: Combined text from all user's resumes

    Returns:
        Tuple of (should_keep, baseline_score, reason):
        - should_keep: Boolean indicating if job passes filters
        - baseline_score: Integer score from 1-100
        - reason: String explanation of filtering decision

    Example:
        keep, score, reason = ai_filter_and_score(job, resume_text)
        if keep:
            print(f"Job scored {score}/100: {reason}")
    """
    client = anthropic.Anthropic()

    # Generate location filter prompt from user's config
    location_filter = CONFIG.get_location_filter_prompt()

    # Get experience level preferences
    exp_level = CONFIG.experience_level
    exclude_keywords = CONFIG.exclude_keywords

    # Build exclusion keyword string for prompt
    exclude_str = ", ".join(exclude_keywords) if exclude_keywords else "None"

    prompt = f"""Analyze this job for filtering and baseline scoring.

CANDIDATE'S RESUME:
{resume_text}

JOB:
Title: {job.get('title', 'Unknown')}
Company: {job.get('company', 'Unknown')}
Location: {job.get('location', 'Unknown')}
Brief Description: {(job.get('raw_text') or 'No description available')[:500]}

INSTRUCTIONS:
1. LOCATION FILTER:
{location_filter}

2. TITLE FILTER: Auto-reject if title contains: {exclude_str}

3. SKILL LEVEL: Keep ALL levels but score appropriately:
   - Entry-level/Junior roles: Give lower score (30-50) but KEEP if candidate has {exp_level.get('min_years', 1)}+ years
   - Mid-level matching candidate's {exp_level.get('current_level', 'mid')} level: High score (60-85)
   - Senior roles slightly above resume: Keep with moderate score (50-70)
   - ONLY filter if extremely mismatched: VP/Director/C-level roles, or requires {exp_level.get('max_years', 10)}+ more years than candidate has

4. BASELINE SCORE (1-100):
   - Location: Use score bonuses from location preferences (Remote=100, primary city=95, etc.)
   - Seniority: Perfect match=+20, Entry-level=-15, Slightly senior=+0, Too senior=-30
   - Company: Top tier (FAANG/unicorn)=+10, Well-known=+5, Startup=+3, Unknown=+0
   - Tech stack overlap: High=+15, Medium=+5, Low=-10

Return JSON only:
{{
    "keep": <bool>,
    "baseline_score": <1-100>,
    "filter_reason": "kept: good location match" OR "filtered: outside target location",
    "location_match": "remote|primary_location|secondary_location|excluded",
    "skill_level_match": "entry_level|good_fit|slightly_senior|too_senior"
}}
"""
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}]
        )
        match = re.search(r'\{[\s\S]*\}', response.content[0].text)
        if match:
            result = json.loads(match.group())
            return (
                result.get('keep', False),
                result.get('baseline_score', 0),
                result.get('filter_reason', 'unknown')
            )
    except Exception as e:
        print(f"AI filter error: {e}")
    
    # Default: keep but low score
    return (True, 30, "filter error - kept by default")

def analyze_job(job: Dict, resume_text: str) -> Dict:
    """
    Perform detailed job qualification analysis using Claude AI.

    This is the "full analysis" that runs after a job passes baseline filtering.
    Provides:
    - Detailed qualification score (1-100)
    - Specific strengths that match the role
    - Gaps or missing requirements
    - Honest recommendation on whether to apply
    - Which resume variant to use

    Args:
        job: Job dictionary with title, company, location, and details
        resume_text: Combined text from all user's resumes

    Returns:
        Dictionary containing:
        - qualification_score: Detailed score from 1-100
        - should_apply: Boolean recommendation
        - strengths: List of matching skills/experience
        - gaps: List of missing requirements
        - recommendation: 2-3 sentence honest assessment
        - resume_to_use: Which resume variant to submit (backend|cloud|fullstack)
    """
    client = anthropic.Anthropic()
    
    prompt = f"""Analyze job fit with strict accuracy. Respond ONLY with valid JSON.

CANDIDATE'S RESUME:
{resume_text}

JOB LISTING:
Title: {job['title']}
Company: {job['company']}
Location: {job['location']}
Details: {job['raw_text']}

CRITICAL INSTRUCTIONS:
1. ONLY mention job titles/roles the candidate has ACTUALLY held (check resume carefully)
2. ONLY cite technologies/skills explicitly listed in resume
3. should_apply = true ONLY if qualification_score >= 65 AND no major dealbreakers
4. Dealbreakers: wrong tech stack, requires 5+ years when candidate has 2, senior leadership role

SCORING RUBRIC:
- 80-100: Strong match, most requirements met, similar past roles
- 60-79: Good match, can do the job with minor gaps
- 40-59: Partial match, significant skill gaps but learnable
- 1-39: Weak match, wrong seniority/stack/domain

Return JSON:
{{
    "qualification_score": <1-100>,
    "should_apply": <bool>,
    "strengths": ["actual skills from resume that match", "relevant past experience"],
    "gaps": ["missing requirements", "areas to improve"],
    "recommendation": "2-3 sentence honest assessment",
    "resume_to_use": "backend|cloud|fullstack"
}}
"""
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        match = re.search(r'\{[\s\S]*\}', response.content[0].text)
        return json.loads(match.group()) if match else {}
    except Exception as e:
        print(f"Analysis error: {e}")
        return {"qualification_score": 0, "should_apply": False, "recommendation": str(e)}

def generate_cover_letter(job: Dict, resume_text: str) -> str:
    """
    Generate a tailored cover letter using Claude AI.

    Creates a personalized cover letter based on:
    - Job requirements and company
    - Candidate's resume and verified experience
    - Previous AI analysis strengths

    The cover letter follows professional best practices:
    - 3-4 paragraphs, under 350 words
    - Only cites actual resume content (no extrapolation)
    - Includes specific examples and metrics
    - Professional but enthusiastic tone

    Args:
        job: Job dictionary with title, company, location, and analysis
        resume_text: Combined text from all user's resumes

    Returns:
        Formatted cover letter text ready to use
    """
    client = anthropic.Anthropic()
    analysis = json.loads(job['analysis']) if job['analysis'] else {}
    
    prompt = f"""Write a tailored cover letter (3-4 paragraphs, under 350 words).

JOB: {job['title']} at {job['company']}
Details: {job['raw_text']}

CANDIDATE RESUME:
{resume_text}

STRENGTHS: {', '.join(analysis.get('strengths', []))}

Write the cover letter now:"""
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text.strip()
    except Exception as e:
        return f"Error: {e}"

# ============== Scoring & Sorting ==============
def calculate_weighted_score(baseline_score: int, email_date: str) -> float:
    """
    Calculate weighted score combining qualification and recency.

    Jobs are sorted by a weighted score that considers both:
    - How well you qualify (70% weight)
    - How recent the posting is (30% weight)

    This ensures high-quality matches rise to the top, while still
    prioritizing newer opportunities over old ones.

    Recency scoring:
    - Posted today: 100 points
    - Linear decay: Loses ~3.33 points per day
    - After 30 days: 0 recency points

    Args:
        baseline_score: AI-generated qualification score (1-100)
        email_date: ISO format date string when job was posted/received

    Returns:
        Weighted score as a float (e.g., 85.67)

    Example:
        - Job with score 90 posted today: 90*0.7 + 100*0.3 = 93.0
        - Job with score 90 posted 10 days ago: 90*0.7 + 66.7*0.3 = 83.0
    """
    # Calculate recency score: 100 for today, linear decay to 0 over 30 days
    try:
        date_obj = datetime.fromisoformat(email_date)
        days_old = (datetime.now() - date_obj).days
        recency_score = max(0, 100 - (days_old * 3.33))  # ~3.33 points lost per day
    except:
        recency_score = 0  # Default to 0 if date parsing fails

    # 70% qualification, 30% recency
    weighted = (baseline_score * 0.7) + (recency_score * 0.3)
    return round(weighted, 2)

# ============== Flask Routes ==============
DASHBOARD_HTML = '''
<!DOCTYPE html>
<html>
<head>
    <title>Hammy the Hire Tracker</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 min-h-screen">
    <div class="max-w-6xl mx-auto p-6">
        <div class="flex justify-between items-center mb-6">
            <div>
                <h1 class="text-3xl font-bold">ðŸ· Hammy the Hire Tracker</h1>
                <p class="text-gray-600">Go HAM on your job search!</p>
            </div>
            <div class="space-x-2">
                <button onclick="scanEmails()" class="bg-blue-600 text-white px-4 py-2 rounded hover:bg-blue-700">
                    ðŸ“§ Scan Gmail
                </button>
                <button onclick="scanWWR()" class="bg-green-600 text-white px-4 py-2 rounded hover:bg-green-700">
                    ðŸŒ Scan WWR
                </button>
                <button onclick="analyzeAll()" class="bg-purple-600 text-white px-4 py-2 rounded hover:bg-purple-700">
                    ðŸ¤– Analyze All
                </button>
                <button onclick="scanFollowups()" class="bg-orange-600 text-white px-4 py-2 rounded hover:bg-orange-700">
                    ðŸ“¬ Scan Follow-Ups
                </button>
            </div>
        </div>
        
        <div class="mb-6 border-b">
            <button onclick="showTab('jobs')" id="tab-jobs" class="px-4 py-2 font-semibold border-b-2 border-blue-600">
                Jobs
            </button>
            <button onclick="showTab('followups')" id="tab-followups" class="px-4 py-2 font-semibold text-gray-600">
                ðŸ“§ Follow-Ups
            </button>
            <button onclick="showTab('watchlist')" id="tab-watchlist" class="px-4 py-2 font-semibold text-gray-600">
                Watchlist
            </button>
        </div>
        
        <div id="jobs-tab">
            <div id="stats" class="grid grid-cols-5 gap-4 mb-6"></div>
            
            <div class="mb-4 flex gap-4">
                <input type="text" id="search" placeholder="Search..." 
                       class="flex-1 px-4 py-2 border rounded" onkeyup="filterJobs()">
                <select id="statusFilter" class="px-4 py-2 border rounded" onchange="loadJobs()">
                    <option value="">All Statuses</option>
                    <option value="new">New</option>
                    <option value="interested">Interested</option>
                    <option value="applied">Applied</option>
                    <option value="passed">Passed</option>
                </select>
                <select id="minScore" class="px-4 py-2 border rounded" onchange="loadJobs()">
                    <option value="0">All Scores</option>
                    <option value="80">80+</option>
                    <option value="60">60+</option>
                    <option value="40">40+</option>
                </select>
            </div>
            
            <div id="jobs" class="space-y-3"></div>
        </div>

        <div id="followups-tab" class="hidden">
            <div id="followup-stats" class="grid grid-cols-5 gap-4 mb-6"></div>
            <div id="followups" class="space-y-3"></div>
        </div>

        <div id="watchlist-tab" class="hidden">
            <div class="bg-white rounded-lg shadow p-6 mb-4">
                <h2 class="text-xl font-bold mb-4">Add Company to Watchlist</h2>
                <div class="space-y-3">
                    <input type="text" id="watch-company" placeholder="Company name" 
                           class="w-full px-4 py-2 border rounded">
                    <input type="url" id="watch-url" placeholder="Careers page URL" 
                           class="w-full px-4 py-2 border rounded">
                    <textarea id="watch-notes" placeholder="Notes (e.g., 'Not hiring now, check Q2')" 
                              class="w-full px-4 py-2 border rounded" rows="3"></textarea>
                    <button onclick="addToWatchlist()" class="bg-blue-600 text-white px-4 py-2 rounded">
                        Add to Watchlist
                    </button>
                </div>
            </div>
            
            <div id="watchlist-items" class="space-y-3"></div>
        </div>
    </div>
    
    <script>
        let allJobs = [];
        let currentTab = 'jobs';
        
        function showTab(tab) {
            currentTab = tab;
            document.getElementById('jobs-tab').classList.toggle('hidden', tab !== 'jobs');
            document.getElementById('followups-tab').classList.toggle('hidden', tab !== 'followups');
            document.getElementById('watchlist-tab').classList.toggle('hidden', tab !== 'watchlist');

            document.getElementById('tab-jobs').className = tab === 'jobs'
                ? 'px-4 py-2 font-semibold border-b-2 border-blue-600'
                : 'px-4 py-2 font-semibold text-gray-600';
            document.getElementById('tab-followups').className = tab === 'followups'
                ? 'px-4 py-2 font-semibold border-b-2 border-blue-600'
                : 'px-4 py-2 font-semibold text-gray-600';
            document.getElementById('tab-watchlist').className = tab === 'watchlist'
                ? 'px-4 py-2 font-semibold border-b-2 border-blue-600'
                : 'px-4 py-2 font-semibold text-gray-600';

            if (tab === 'followups') loadFollowups();
            if (tab === 'watchlist') loadWatchlist();
        }
        
        function formatDate(dateStr) {
            if (!dateStr) return '';
            try {
                const d = new Date(dateStr);
                return d.toLocaleDateString('en-US', { month: 'short', day: 'numeric' });
            } catch {
                return '';
            }
        }
        
        async function loadJobs() {
            const status = document.getElementById('statusFilter').value;
            const minScore = document.getElementById('minScore').value;
            const params = new URLSearchParams({status, min_score: minScore});
            
            const res = await fetch('/api/jobs?' + params);
            const data = await res.json();
            allJobs = data.jobs;
            renderJobs(allJobs);
            renderStats(data.stats);
        }
        
        function renderStats(stats) {
            document.getElementById('stats').innerHTML = `
                <div class="bg-white p-4 rounded shadow text-center">
                    <div class="text-2xl font-bold">${stats.total}</div>
                    <div class="text-gray-500 text-sm">Total</div>
                </div>
                <div class="bg-blue-50 p-4 rounded shadow text-center">
                    <div class="text-2xl font-bold">${stats.new}</div>
                    <div class="text-gray-500 text-sm">New</div>
                </div>
                <div class="bg-yellow-50 p-4 rounded shadow text-center">
                    <div class="text-2xl font-bold">${stats.interested}</div>
                    <div class="text-gray-500 text-sm">Interested</div>
                </div>
                <div class="bg-green-50 p-4 rounded shadow text-center">
                    <div class="text-2xl font-bold">${stats.applied}</div>
                    <div class="text-gray-500 text-sm">Applied</div>
                </div>
                <div class="bg-purple-50 p-4 rounded shadow text-center">
                    <div class="text-2xl font-bold">${Math.round(stats.avg_score)}</div>
                    <div class="text-gray-500 text-sm">Avg Score</div>
                </div>
            `;
        }
        
        function filterJobs() {
            const search = document.getElementById('search').value.toLowerCase();
            const filtered = allJobs.filter(j => 
                j.title.toLowerCase().includes(search) || 
                (j.company || '').toLowerCase().includes(search)
            );
            renderJobs(filtered);
        }
        
        function renderJobs(jobs) {
            const container = document.getElementById('jobs');
            container.innerHTML = jobs.map(job => {
                const analysis = job.analysis ? JSON.parse(job.analysis) : {};
                const scoreColor = job.baseline_score >= 80 ? 'bg-green-500' : 
                                   job.baseline_score >= 60 ? 'bg-blue-500' : 
                                   job.baseline_score >= 40 ? 'bg-yellow-500' : 'bg-gray-300';
                
                // Status colors
                const statusColors = {
                    'new': 'bg-gray-100 border-gray-300',
                    'interested': 'bg-blue-50 border-blue-300',
                    'applied': 'bg-green-50 border-green-400',
                    'interviewing': 'bg-purple-50 border-purple-300',
                    'passed': 'bg-gray-50 border-gray-200',
                    'rejected': 'bg-red-50 border-red-200'
                };
                
                const statusColor = statusColors[job.status] || statusColors['new'];
                const viewedStyle = job.viewed ? 'opacity-90 bg-gray-100' : '';
                
                return `
                <div class="bg-white ${viewedStyle} rounded-lg shadow p-4 border-l-4 ${statusColor}">
                    <div class="flex justify-between items-start">
                        <div class="flex-1">
                            <div class="flex items-center gap-2 mb-1">
                                <span class="${scoreColor} text-white px-2 py-1 rounded-full text-sm font-bold">
                                    ${job.baseline_score || 'â€”'}
                                </span>
                                <h3 class="font-semibold">${job.title}</h3>
                            </div>
                            <p class="text-gray-600 text-sm">${job.company || 'Unknown'} â€¢ ${job.location || ''}</p>
                            <p class="text-gray-400 text-xs">${job.source} â€¢ ${formatDate(job.email_date)}</p>
                            
                            ${analysis.recommendation ? `
                            <div class="mt-2 p-2 bg-blue-50 border-l-2 border-blue-400 rounded text-sm">
                                <strong class="text-blue-900">AI Insight:</strong>
                                <p class="text-gray-700 mt-1">${analysis.recommendation}</p>
                            </div>
                            ` : ''}
                        </div>
                        <div class="flex items-center gap-2">
                            <select onchange="updateStatus('${job.job_id}', this.value)" 
                                    class="text-sm border rounded px-2 py-1">
                                ${['new','interested','applied','interviewing','passed','rejected'].map(s => 
                                    `<option value="${s}" ${job.status === s ? 'selected' : ''}>${s}</option>`
                                ).join('')}
                            </select>
                            <button onclick="addToWatchlistFromJob('${job.company}', '${job.url}')" 
                                    class="text-yellow-600 hover:text-yellow-700 p-1" title="Add to Watchlist">
                                â­
                            </button>
                            <button onclick="hideJob('${job.job_id}')" 
                                    class="text-gray-400 hover:text-red-600 p-1" title="Hide">
                                âœ•
                            </button>
                            <a href="${job.url}" target="_blank" class="text-blue-600 hover:underline text-sm"
                               onclick="markViewed('${job.job_id}')">View</a>
                        </div>
                    </div>
                    
                    ${analysis.strengths ? `
                    <details class="mt-3">
                        <summary class="cursor-pointer text-sm text-gray-500">Full Analysis</summary>
                        
                        <div class="mt-2 grid grid-cols-2 gap-4 text-sm">
                            <div>
                                <h4 class="font-semibold text-green-700">Strengths</h4>
                                <ul class="list-disc list-inside">${analysis.strengths.map(s => `<li>${s}</li>`).join('')}</ul>
                            </div>
                            <div>
                                <h4 class="font-semibold text-red-700">Gaps</h4>
                                <ul class="list-disc list-inside">${(analysis.gaps || []).map(g => `<li>${g}</li>`).join('')}</ul>
                            </div>
                        </div>
                        
                        ${job.cover_letter ? `
                        <div class="mt-3">
                            <h4 class="font-semibold">Cover Letter</h4>
                            <pre class="bg-gray-50 p-3 rounded text-sm whitespace-pre-wrap mt-1">${job.cover_letter}</pre>
                        </div>
                        ` : `
                        <button onclick="generateCoverLetter('${job.job_id}')" 
                                class="mt-2 bg-purple-600 text-white px-3 py-1 rounded text-sm">
                            Generate Cover Letter
                        </button>
                        `}
                    </details>
                    ` : ''}
                </div>
                `;
            }).join('');
        }
        
        async function loadWatchlist() {
            const res = await fetch('/api/watchlist');
            const data = await res.json();
            
            const container = document.getElementById('watchlist-items');
            if (data.items.length === 0) {
                container.innerHTML = '<p class="text-gray-500 text-center py-8">No companies on watchlist yet</p>';
                return;
            }
            
            container.innerHTML = data.items.map(item => `
                <div class="bg-white rounded-lg shadow p-4">
                    <div class="flex justify-between items-start">
                        <div class="flex-1">
                            <h3 class="font-semibold text-lg">${item.company}</h3>
                            <a href="${item.url}" target="_blank" class="text-blue-600 hover:underline text-sm">
                                ${item.url}
                            </a>
                            ${item.notes ? `<p class="text-gray-600 text-sm mt-2">${item.notes}</p>` : ''}
                            <p class="text-gray-400 text-xs mt-1">Added ${formatDate(item.created_at)}</p>
                        </div>
                        <button onclick="removeFromWatchlist(${item.id})" 
                                class="text-red-600 hover:text-red-700">
                            Remove
                        </button>
                    </div>
                </div>
            `).join('');
        }
        
        async function addToWatchlist() {
            const company = document.getElementById('watch-company').value.trim();
            const url = document.getElementById('watch-url').value.trim();
            const notes = document.getElementById('watch-notes').value.trim();
            
            if (!company) {
                alert('Company name required');
                return;
            }
            
            await fetch('/api/watchlist', {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({company, url, notes})
            });
            
            document.getElementById('watch-company').value = '';
            document.getElementById('watch-url').value = '';
            document.getElementById('watch-notes').value = '';
            
            loadWatchlist();
        }
        
        function addToWatchlistFromJob(company, url) {
            document.getElementById('watch-company').value = company;
            document.getElementById('watch-url').value = url;
            showTab('watchlist');
        }
        
        async function removeFromWatchlist(id) {
            console.log('[Dashboard] Removing from watchlist:', id);
            await fetch(`/api/watchlist/${id}`, {method: 'DELETE'});
            loadWatchlist();
        }
        
        async function scanWWR() {
            const btn = event.target;
            const originalText = btn.textContent;
            btn.disabled = true;
            btn.textContent = 'Scanning WWR...';
            
            try {
                await fetch('/api/wwr', {method: 'POST'});
                // Poll for updates
                setTimeout(loadJobs, 3000);
                setTimeout(loadJobs, 10000);
                setTimeout(loadJobs, 20000);
            } catch (err) {
                console.error('WWR scan failed:', err);
            }
            
            btn.disabled = false;
            btn.textContent = originalText;
        }
        
        async function scanEmails() {
            const btn = event.target;
            btn.disabled = true;
            btn.textContent = 'Scanning...';
            await fetch('/api/scan', {method: 'POST'});
            await loadJobs();
            btn.disabled = false;
            btn.textContent = 'ðŸ“§ Scan Gmail';
        }
        
        async function analyzeAll() {
            const btn = event.target;
            btn.disabled = true;
            btn.textContent = 'Analyzing...';
            await fetch('/api/analyze', {method: 'POST'});
            await loadJobs();
            btn.disabled = false;
            btn.textContent = 'ðŸ¤– Analyze All';
        }
        
        async function updateStatus(jobId, status) {
            await fetch(`/api/jobs/${jobId}`, {
                method: 'PATCH',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({status})
            });
            loadJobs();
        }
        
        async function markViewed(jobId) {
            await fetch(`/api/jobs/${jobId}`, {
                method: 'PATCH',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({viewed: 1})
            });
            setTimeout(() => loadJobs(), 500);
        }
        
        async function hideJob(jobId) {
            console.log('[Dashboard] Hiding job:', jobId);
            await fetch(`/api/jobs/${jobId}`, {
                method: 'PATCH',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({status: 'hidden'})
            });
            loadJobs();
        }
        
        async function generateCoverLetter(jobId) {
            event.target.disabled = true;
            event.target.textContent = 'Generating...';
            await fetch(`/api/jobs/${jobId}/cover-letter`, {method: 'POST'});
            loadJobs();
        }

        async function scanFollowups() {
            const btn = event.target;
            btn.disabled = true;
            btn.textContent = 'Scanning Follow-Ups...';
            try {
                const res = await fetch('/api/scan-followups', {method: 'POST'});
                const data = await res.json();
                alert(`Found ${data.found} follow-ups!\\n${data.new} new\\n${data.updated_jobs} jobs updated`);
                await loadFollowups();
                await loadJobs();
            } finally {
                btn.disabled = false;
                btn.textContent = 'ðŸ“¬ Scan Follow-Ups';
            }
        }

        async function loadFollowups() {
            const res = await fetch('/api/followups');
            const data = await res.json();

            // Display statistics
            const stats = data.stats;
            document.getElementById('followup-stats').innerHTML = `
                <div class="bg-white rounded-lg shadow p-4 text-center">
                    <div class="text-2xl font-bold">${stats.total}</div>
                    <div class="text-sm text-gray-600">Total Responses</div>
                </div>
                <div class="bg-green-100 rounded-lg shadow p-4 text-center">
                    <div class="text-2xl font-bold text-green-700">ðŸŽ‰ ${stats.interviews}</div>
                    <div class="text-sm text-gray-600">Interviews</div>
                </div>
                <div class="bg-blue-100 rounded-lg shadow p-4 text-center">
                    <div class="text-2xl font-bold text-blue-700">ðŸŽ ${stats.offers}</div>
                    <div class="text-sm text-gray-600">Offers</div>
                </div>
                <div class="bg-red-100 rounded-lg shadow p-4 text-center">
                    <div class="text-2xl font-bold text-red-700">ðŸ˜ž ${stats.rejections}</div>
                    <div class="text-sm text-gray-600">Rejections</div>
                </div>
                <div class="bg-purple-100 rounded-lg shadow p-4 text-center">
                    <div class="text-2xl font-bold text-purple-700">${stats.response_rate}%</div>
                    <div class="text-sm text-gray-600">Response Rate</div>
                </div>
            `;

            // Display follow-ups
            const followupsHTML = data.followups.map(f => {
                const typeIcons = {
                    'interview': 'ðŸŽ‰',
                    'rejection': 'ðŸ˜ž',
                    'offer': 'ðŸŽ',
                    'assessment': 'ðŸ“‹',
                    'received': 'âœ…',
                    'update': 'ðŸ“§'
                };
                const typeColors = {
                    'interview': 'bg-green-100 border-green-300',
                    'rejection': 'bg-red-100 border-red-300',
                    'offer': 'bg-blue-100 border-blue-300',
                    'assessment': 'bg-purple-100 border-purple-300',
                    'received': 'bg-gray-100 border-gray-300',
                    'update': 'bg-yellow-100 border-yellow-300'
                };

                const icon = typeIcons[f.type] || 'ðŸ“§';
                const color = typeColors[f.type] || 'bg-gray-100 border-gray-300';
                const matchBadge = f.job_id ? '<span class="text-xs bg-blue-500 text-white px-2 py-1 rounded">Matched</span>' : '';
                const spamBadge = f.in_spam ? '<span class="text-xs bg-red-500 text-white px-2 py-1 rounded">Was in Spam!</span>' : '';

                return `
                    <div class="bg-white rounded-lg shadow border-l-4 ${color} p-4">
                        <div class="flex justify-between items-start mb-2">
                            <div>
                                <div class="flex items-center gap-2">
                                    <span class="text-2xl">${icon}</span>
                                    <h3 class="font-bold text-lg">${f.type.toUpperCase()}: ${f.company}</h3>
                                    ${matchBadge}
                                    ${spamBadge}
                                </div>
                                <p class="text-sm text-gray-600">${f.subject}</p>
                            </div>
                            <div class="text-right">
                                <div class="text-sm text-gray-500">${formatDate(f.email_date)}</div>
                                ${f.title ? `<div class="text-xs text-blue-600 mt-1">${f.title}</div>` : ''}
                            </div>
                        </div>
                        <p class="text-sm text-gray-700 mt-2">${f.snippet}</p>
                        ${f.url ? `<a href="${f.url}" target="_blank" class="text-blue-600 text-sm hover:underline mt-2 inline-block">View Job â†’</a>` : ''}
                    </div>
                `;
            }).join('');

            document.getElementById('followups').innerHTML = followupsHTML ||
                '<div class="bg-white rounded-lg shadow p-8 text-center text-gray-500">No follow-ups yet. Click "ðŸ“¬ Scan Follow-Ups" to check your email!</div>';
        }

        loadJobs();
    </script>
</body>
</html>
'''

@app.route('/')
def dashboard():
    # Serve the built React app from dist folder
    dist_index = APP_DIR / 'dist' / 'index.html'
    if dist_index.exists():
        return dist_index.read_text()
    else:
        return "Frontend not built! Run 'npm run build' first.", 500

@app.route('/api/jobs')
def get_jobs():
    status = request.args.get('status', '')
    min_score = int(request.args.get('min_score', 0))
    show_hidden = request.args.get('show_hidden', 'false') == 'true'
    
    conn = get_db()
    query = "SELECT * FROM jobs WHERE is_filtered = 0"
    params = []
    
    if not show_hidden:
        query += " AND status != 'hidden'"
    
    if status:
        query += " AND status = ?"
        params.append(status)
    if min_score:
        query += " AND baseline_score >= ?"
        params.append(min_score)
    
    # Fetch all matching jobs
    jobs = [dict(row) for row in conn.execute(query, params).fetchall()]
    
    # Calculate weighted scores and sort
    for job in jobs:
        job['weighted_score'] = calculate_weighted_score(
            job.get('baseline_score', 0), 
            job.get('email_date', job.get('created_at', ''))
        )
    
    jobs.sort(key=lambda x: x['weighted_score'], reverse=True)
    
    # Stats
    all_jobs = [dict(row) for row in conn.execute("SELECT status, baseline_score FROM jobs WHERE is_filtered = 0 AND status != 'hidden'").fetchall()]
    stats = {
        'total': len(all_jobs),
        'new': len([j for j in all_jobs if j['status'] == 'new']),
        'interested': len([j for j in all_jobs if j['status'] == 'interested']),
        'applied': len([j for j in all_jobs if j['status'] == 'applied']),
        'avg_score': sum(j['baseline_score'] or 0 for j in all_jobs) / len(all_jobs) if all_jobs else 0
    }
    
    conn.close()
    return jsonify({'jobs': jobs, 'stats': stats})

@app.route('/api/jobs/<job_id>', methods=['PATCH'])
def update_job(job_id):
    data = request.json
    conn = get_db()

    # Build update query dynamically
    allowed_fields = ['status', 'notes', 'viewed', 'applied_date', 'interview_date']
    updates = []
    params = []

    for field in allowed_fields:
        if field in data:
            updates.append(f"{field} = ?")
            params.append(data[field])

    if updates:
        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(job_id)

        query = f"UPDATE jobs SET {', '.join(updates)} WHERE job_id = ?"
        conn.execute(query, params)

        # If status is being updated and this job is linked to an external application, sync it
        if 'status' in data:
            conn.execute(
                "UPDATE external_applications SET status = ?, updated_at = ? WHERE job_id = ?",
                (data['status'], datetime.now().isoformat(), job_id)
            )
            print(f"[Backend] Synced status '{data['status']}' to linked external application")

        conn.commit()

    conn.close()
    return jsonify({'success': True})

@app.route('/api/jobs/<job_id>', methods=['DELETE'])
def delete_job(job_id):
    """Delete a job from the database."""
    conn = get_db()

    # Delete the job
    conn.execute("DELETE FROM jobs WHERE job_id = ?", (job_id,))

    # Also delete any linked external applications
    conn.execute("DELETE FROM external_applications WHERE job_id = ?", (job_id,))

    # Delete any resume usage logs for this job
    conn.execute("DELETE FROM resume_usage_log WHERE job_id = ?", (job_id,))

    conn.commit()
    conn.close()

    return jsonify({'success': True})

@app.route('/api/scan', methods=['POST'])
def api_scan():
    """Scan emails with AI filtering and baseline scoring."""
    jobs = scan_emails()
    resume_text = load_resumes()
    
    if not resume_text:
        return jsonify({'error': 'No resumes found. Add .txt/.md files to resumes/ folder'}), 400
    
    conn = get_db()
    new_count = 0
    filtered_count = 0
    duplicate_count = 0
    
    try:
        for job in jobs:
            # Check if job already exists in DB (by job_id, URL, or company+title)
            existing = conn.execute("""
                SELECT 1 FROM jobs
                WHERE job_id = ?
                OR url = ?
                OR (company = ? AND title = ?)
            """, (job['job_id'], job['url'], job['company'], job['title'])).fetchone()
            if existing:
                duplicate_count += 1
                print(f"â­ï¸  Skipping duplicate: {job['title'][:50]}")
                continue
            
            # AI filter and baseline score
            keep, baseline_score, reason = ai_filter_and_score(job, resume_text)
            
            if keep:
                conn.execute('''
                    INSERT INTO jobs (job_id, title, company, location, url, source, raw_text, 
                                     baseline_score, created_at, updated_at, email_date, is_filtered)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0)
                ''', (job['job_id'], job['title'], job['company'], job['location'], 
                      job['url'], job['source'], job['raw_text'], baseline_score,
                      job['created_at'], datetime.now().isoformat(), job.get('email_date', job['created_at'])))
                conn.commit()  # Commit immediately
                new_count += 1
                print(f"âœ“ Kept: {job['title'][:50]} - Score {baseline_score} - {reason}")
            else:
                # Store filtered jobs but mark them
                conn.execute('''
                    INSERT INTO jobs (job_id, title, company, location, url, source, raw_text, 
                                     baseline_score, created_at, updated_at, email_date, is_filtered, notes)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 1, ?)
                ''', (job['job_id'], job['title'], job['company'], job['location'], 
                      job['url'], job['source'], job['raw_text'], baseline_score,
                      job['created_at'], datetime.now().isoformat(), job.get('email_date', job['created_at']), reason))
                conn.commit()  # Commit immediately
                filtered_count += 1
                print(f"âœ— Filtered: {job['title'][:50]} - {reason}")
    finally:
        conn.close()
    
    print(f"\nðŸ“Š Scan Summary:")
    print(f"   - Found: {len(jobs)} jobs")
    print(f"   - New & kept: {new_count}")
    print(f"   - Filtered: {filtered_count}")
    print(f"   - Duplicates skipped: {duplicate_count}")
    
    return jsonify({
        'found': len(jobs), 
        'new': new_count, 
        'filtered': filtered_count,
        'duplicates': duplicate_count
    })

@app.route('/api/analyze', methods=['POST'])
def api_analyze():
    """Full analysis on jobs that passed baseline filter."""
    resume_text = load_resumes()
    if not resume_text:
        return jsonify({'error': 'No resumes found'}), 400
    
    conn = get_db()
    jobs = [dict(row) for row in conn.execute(
        "SELECT * FROM jobs WHERE is_filtered = 0 AND (score = 0 OR score IS NULL)"
    ).fetchall()]
    
    for job in jobs:
        print(f"Analyzing: {job['title']}")
        analysis = analyze_job(job, resume_text)
        
        # Only change status if it's still 'new', otherwise preserve user's choice
        new_status = job['status']
        if job['status'] == 'new':
            new_status = 'interested' if analysis.get('should_apply') else 'new'
        
        conn.execute(
            "UPDATE jobs SET score = ?, analysis = ?, status = ?, updated_at = ? WHERE job_id = ?",
            (analysis.get('qualification_score', 0), json.dumps(analysis),
             new_status,
             datetime.now().isoformat(), job['job_id'])
        )
        conn.commit()
    
    conn.close()
    return jsonify({'analyzed': len(jobs)})

@app.route('/api/score-jobs', methods=['POST'])
def api_score_jobs():
    """Score all jobs that don't have scores yet based on qualifications and job title match."""
    resume_text = load_resumes()
    if not resume_text:
        return jsonify({'error': 'No resumes found'}), 400

    conn = get_db()
    # Get jobs that don't have scores yet
    jobs = [dict(row) for row in conn.execute(
        "SELECT * FROM jobs WHERE (score = 0 OR score IS NULL) AND is_filtered = 0"
    ).fetchall()]

    scored_count = 0
    for job in jobs:
        try:
            # Use AI to score the job
            _, baseline_score, reason = ai_filter_and_score(job, resume_text)

            # Update job with new score
            conn.execute(
                "UPDATE jobs SET score = ?, notes = ?, updated_at = ? WHERE job_id = ?",
                (baseline_score, reason, datetime.now().isoformat(), job['job_id'])
            )
            conn.commit()
            scored_count += 1
            print(f"âœ“ Scored: {job['title'][:50]} - Score {baseline_score}")
        except Exception as e:
            print(f"Error scoring job {job['job_id']}: {e}")
            continue

    conn.close()
    return jsonify({'scored': scored_count, 'total': len(jobs)})

@app.route('/api/scan-followups', methods=['POST'])
def api_scan_followups():
    """
    Scan Gmail for follow-up emails (interviews, rejections, offers).
    Automatically updates job statuses and stores follow-ups in database.
    """
    followups = scan_followup_emails(days_back=30)

    conn = get_db()
    new_count = 0
    updated_jobs = 0

    try:
        for followup in followups:
            # Check if this follow-up already exists
            existing = conn.execute(
                "SELECT id FROM followups WHERE company = ? AND subject = ? AND email_date = ?",
                (followup['company'], followup['subject'], followup['email_date'])
            ).fetchone()

            if existing:
                continue  # Skip duplicates

            # Insert follow-up into database
            conn.execute(
                '''INSERT INTO followups (company, subject, type, snippet, email_date, job_id, created_at)
                   VALUES (?, ?, ?, ?, ?, ?, ?)''',
                (followup['company'], followup['subject'], followup['type'],
                 followup['snippet'], followup['email_date'], followup['job_id'],
                 datetime.now().isoformat())
            )
            conn.commit()
            new_count += 1

            # Auto-update job status if matched
            if followup['job_id']:
                job = conn.execute("SELECT status FROM jobs WHERE job_id = ?", (followup['job_id'],)).fetchone()

                if job:
                    current_status = job[0]
                    new_status = current_status

                    # Update status based on follow-up type
                    if followup['type'] == 'rejection' and current_status != 'rejected':
                        new_status = 'rejected'
                    elif followup['type'] == 'interview' and current_status not in ['interviewing', 'offered', 'accepted']:
                        new_status = 'interviewing'
                    elif followup['type'] == 'offer' and current_status not in ['offered', 'accepted']:
                        new_status = 'offered'

                    if new_status != current_status:
                        conn.execute(
                            "UPDATE jobs SET status = ?, updated_at = ? WHERE job_id = ?",
                            (new_status, datetime.now().isoformat(), followup['job_id'])
                        )
                        conn.commit()
                        updated_jobs += 1
                        print(f"âœ“ Updated {followup['company']} â†’ {new_status}")

    finally:
        conn.close()

    return jsonify({
        'found': len(followups),
        'new': new_count,
        'updated_jobs': updated_jobs
    })

@app.route('/api/followups', methods=['GET'])
def api_get_followups():
    """Get all follow-up emails with associated job info."""
    conn = get_db()

    followups = conn.execute('''
        SELECT f.*, j.title, j.company as job_company, j.url
        FROM followups f
        LEFT JOIN jobs j ON f.job_id = j.job_id
        ORDER BY f.email_date DESC
        LIMIT 100
    ''').fetchall()

    # Calculate statistics
    stats = {
        'total': len(followups),
        'interviews': len([f for f in followups if f['type'] == 'interview']),
        'rejections': len([f for f in followups if f['type'] == 'rejection']),
        'offers': len([f for f in followups if f['type'] == 'offer']),
        'assessments': len([f for f in followups if f['type'] == 'assessment']),
    }

    # Calculate response rate
    applied_count = conn.execute("SELECT COUNT(*) FROM jobs WHERE status IN ('applied', 'interviewing', 'offered', 'rejected')").fetchone()[0]
    if applied_count > 0:
        stats['response_rate'] = round((stats['total'] / applied_count) * 100, 1)
    else:
        stats['response_rate'] = 0

    conn.close()

    return jsonify({
        'followups': [dict(row) for row in followups],
        'stats': stats
    })

@app.route('/api/jobs/<job_id>/cover-letter', methods=['POST'])
def api_cover_letter(job_id):
    resume_text = load_resumes()
    conn = get_db()
    job = dict(conn.execute("SELECT * FROM jobs WHERE job_id = ?", (job_id,)).fetchone())
    
    cover_letter = generate_cover_letter(job, resume_text)
    conn.execute(
        "UPDATE jobs SET cover_letter = ?, updated_at = ? WHERE job_id = ?",
        (cover_letter, datetime.now().isoformat(), job_id)
    )
    conn.commit()
    conn.close()
    return jsonify({'cover_letter': cover_letter})

@app.route('/api/capture', methods=['POST'])
def api_capture():
    """Receive job from browser extension."""
    data = request.json
    
    url = clean_job_url(data.get('url', ''))
    title = data.get('title', '')
    company = data.get('company', '')
    location = data.get('location', 'Remote')
    description = data.get('description', '')
    source = data.get('source', 'extension')
    
    # Auto-detect source from URL
    if 'linkedin.com' in url:
        source = 'linkedin'
    elif 'indeed.com' in url:
        source = 'indeed'
    elif 'weworkremotely.com' in url:
        source = 'weworkremotely'
    
    if not url or not title:
        return jsonify({'error': 'url and title required'}), 400
    
    job_id = generate_job_id(url, title, company)
    
    conn = get_db()
    existing = conn.execute("SELECT 1 FROM jobs WHERE job_id = ?", (job_id,)).fetchone()
    
    if existing:
        conn.execute('''
            UPDATE jobs SET description = ?, raw_text = ?, updated_at = ?
            WHERE job_id = ? AND (description IS NULL OR description = '')
        ''', (description[:5000], description[:2000], datetime.now().isoformat(), job_id))
        conn.commit()
        conn.close()
        return jsonify({'status': 'updated', 'job_id': job_id})
    
    # New job from extension - add with baseline score
    resume_text = load_resumes()
    baseline_score = 50  # Default
    
    if resume_text:
        temp_job = {
            'title': title, 'company': company, 'location': location,
            'raw_text': description[:500] if description else title
        }
        keep, baseline_score, reason = ai_filter_and_score(temp_job, resume_text)
    
    conn.execute('''
        INSERT INTO jobs (job_id, title, company, location, url, source, description, raw_text, 
                         baseline_score, created_at, updated_at, is_filtered)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0)
    ''', (job_id, title[:200], company[:100], location[:100], url, source, 
          description[:5000], description[:2000], baseline_score,
          datetime.now().isoformat(), datetime.now().isoformat()))
    conn.commit()
    conn.close()
    
    return jsonify({'status': 'created', 'job_id': job_id, 'baseline_score': baseline_score})

@app.route('/api/analyze-instant', methods=['POST'])
def api_analyze_instant():
    """Instant analysis for browser extension with strict accuracy."""
    data = request.json
    
    title = data.get('title', '')
    company = data.get('company', 'Unknown')
    description = data.get('description', '')
    
    if not title or not description:
        return jsonify({'error': 'title and description required'}), 400
    
    resume_text = load_resumes()
    if not resume_text:
        return jsonify({'error': 'No resumes found'}), 400
    
    client = anthropic.Anthropic()
    
    prompt = f"""Analyze job fit with STRICT ACCURACY. Only mention roles/skills candidate ACTUALLY has.

CANDIDATE'S RESUME:
{resume_text}

JOB LISTING:
Title: {title}
Company: {company}
Description: {description[:2000]}

CRITICAL RULES:
1. ONLY cite job titles the candidate has held (check resume carefully)
2. ONLY mention technologies/tools explicitly in resume
3. Do NOT invent experience or extrapolate skills
4. should_apply = true ONLY if score >= 65 AND no major red flags
5. Red flags: requires 5+ years when candidate has 2, wrong tech stack entirely, senior leadership position

SCORING:
- 80-100: Strong match, candidate has done similar work
- 60-79: Good match, meets most requirements with minor gaps
- 40-59: Partial match, missing key skills but could learn
- 20-39: Weak match, significant gaps in experience/skills
- 1-19: Very poor match, wrong level or domain entirely

Return ONLY valid JSON:
{{
    "qualification_score": <1-100>,
    "should_apply": <bool>,
    "strengths": ["actual matching skills from resume", "relevant experience candidate has"],
    "gaps": ["specific missing requirements", "areas to develop"],
    "recommendation": "Honest 2-3 sentence assessment based on actual resume fit",
    "resume_to_use": "backend|cloud|fullstack"
}}
"""
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        response_text = response.content[0].text
        match = re.search(r'\{[\s\S]*\}', response_text)
        
        if match:
            analysis = json.loads(match.group())
        else:
            raise ValueError("No JSON in response")
        
        return jsonify({'analysis': analysis, 'job': {'title': title, 'company': company}})
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/wwr', methods=['POST'])
def api_scan_wwr():
    """Scan WWR with AI filtering."""
    print("ðŸŒ Starting WWR scan...")
    jobs = fetch_wwr_jobs()
    print(f"ðŸ“¥ Fetched {len(jobs)} jobs from RSS feeds")
    
    resume_text = load_resumes()
    
    if not resume_text:
        return jsonify({'error': 'No resumes found'}), 400
    
    conn = get_db()
    new_count = 0
    filtered_count = 0
    duplicate_count = 0
    
    try:
        for i, job in enumerate(jobs, 1):
            print(f"Processing {i}/{len(jobs)}: {job['title'][:50]}...")
            
            existing = conn.execute("SELECT 1 FROM jobs WHERE job_id = ?", (job['job_id'],)).fetchone()
            if existing:
                duplicate_count += 1
                print(f"  â­ï¸  Duplicate")
                continue
            
            keep, baseline_score, reason = ai_filter_and_score(job, resume_text)
            
            if keep:
                conn.execute('''
                    INSERT INTO jobs (job_id, title, company, location, url, source, raw_text, 
                                     baseline_score, created_at, updated_at, email_date, is_filtered)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0)
                ''', (job['job_id'], job['title'], job['company'], job['location'], 
                      job['url'], job['source'], job.get('description', job['raw_text']), baseline_score,
                      job['created_at'], datetime.now().isoformat(), job.get('email_date', job['created_at'])))
                conn.commit()
                new_count += 1
                print(f"  âœ“ Kept - Score {baseline_score}")
            else:
                conn.execute('''
                    INSERT INTO jobs (job_id, title, company, location, url, source, raw_text, 
                                     baseline_score, created_at, updated_at, email_date, is_filtered, notes)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 1, ?)
                ''', (job['job_id'], job['title'], job['company'], job['location'], 
                      job['url'], job['source'], job.get('description', job['raw_text']), baseline_score,
                      job['created_at'], datetime.now().isoformat(), job.get('email_date', job['created_at']), reason))
                conn.commit()
                filtered_count += 1
                print(f"  âœ— Filtered - {reason[:50]}")
    finally:
        conn.close()
    
    print(f"\nâœ… WWR Scan Complete: {new_count} new, {filtered_count} filtered, {duplicate_count} duplicates")
    return jsonify({'found': len(jobs), 'new': new_count, 'filtered': filtered_count, 'duplicates': duplicate_count})

@app.route('/api/generate-cover-letter', methods=['POST'])
def api_generate_cover_letter():
    """Generate cover letter for extension with accurate resume matching."""
    data = request.json
    job = data.get('job', {})
    analysis = data.get('analysis', {})
    
    resume_text = load_resumes()
    if not resume_text:
        return jsonify({'error': 'No resumes found'}), 400
    
    client = anthropic.Anthropic()
    
    strengths = ', '.join(analysis.get('strengths', []))
    
    prompt = f"""Write a tailored cover letter (3-4 paragraphs, under 350 words).

CRITICAL: Only mention experience and skills the candidate ACTUALLY has from their resume.

CANDIDATE'S RESUME:
{resume_text}

JOB:
Title: {job.get('title')}
Company: {job.get('company')}
Description: {job.get('description', '')[:1000]}

KEY STRENGTHS (verified from resume):
{strengths}

INSTRUCTIONS:
1. ONLY cite projects, roles, and technologies from the resume
2. Use specific examples and metrics from resume
3. Do NOT invent experience or extrapolate skills
4. Keep professional but enthusiastic tone
5. 3-4 paragraphs: opening, 2 body (experience/fit), closing

Write only the cover letter text (no subject line):"""
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        cover_letter = response.content[0].text.strip()
        return jsonify({'cover_letter': cover_letter})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/generate-answer', methods=['POST'])
def api_generate_answer():
    """Generate interview answer with accurate resume references."""
    data = request.json
    job = data.get('job', {})
    question = data.get('question')
    analysis = data.get('analysis', {})
    
    if not question:
        return jsonify({'error': 'Question required'}), 400
    
    resume_text = load_resumes()
    if not resume_text:
        return jsonify({'error': 'No resumes found'}), 400
    
    client = anthropic.Anthropic()
    
    prompt = f"""Generate a strong interview answer using ONLY actual resume content.

QUESTION: {question}

JOB CONTEXT:
Title: {job.get('title')}
Company: {job.get('company')}
Description: {job.get('description', '')[:500]}

CANDIDATE'S RESUME:
{resume_text}

VERIFIED ANALYSIS:
Strengths: {', '.join(analysis.get('strengths', []))}
Gaps: {', '.join(analysis.get('gaps', []))}

CRITICAL RULES:
1. ONLY cite projects, roles, metrics from the actual resume
2. Do NOT invent experience or extrapolate skills
3. Use specific examples with concrete details
4. Be honest about gaps but frame positively
5. Natural, conversational tone (not rehearsed)

Generate 2-3 paragraph answer (150-200 words):"""
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=800,
            messages=[{"role": "user", "content": prompt}]
        )
        answer = response.content[0].text.strip()
        return jsonify({'answer': answer})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/watchlist', methods=['GET'])
def get_watchlist():
    conn = get_db()
    items = [dict(row) for row in conn.execute(
        "SELECT * FROM watchlist ORDER BY created_at DESC"
    ).fetchall()]
    conn.close()
    return jsonify({'items': items})

@app.route('/api/watchlist', methods=['POST'])
def add_watchlist():
    data = request.json
    company = data.get('company', '').strip()
    url = data.get('url', '').strip()
    notes = data.get('notes', '').strip()
    
    if not company:
        return jsonify({'error': 'Company required'}), 400
    
    conn = get_db()
    conn.execute(
        "INSERT INTO watchlist (company, url, notes, created_at) VALUES (?, ?, ?, ?)",
        (company, url, notes, datetime.now().isoformat())
    )
    conn.commit()
    conn.close()
    return jsonify({'success': True})

@app.route('/api/watchlist/<int:watch_id>', methods=['DELETE'])
def delete_watchlist(watch_id):
    conn = get_db()
    conn.execute("DELETE FROM watchlist WHERE id = ?", (watch_id,))
    conn.commit()
    conn.close()
    return jsonify({'success': True})

# ============== Tracked Companies ==============
@app.route('/api/tracked-companies', methods=['GET'])
def get_tracked_companies():
    """Get all tracked companies ordered by most recently added."""
    conn = get_db()
    companies = [dict(row) for row in conn.execute(
        "SELECT * FROM tracked_companies ORDER BY created_at DESC"
    ).fetchall()]
    conn.close()
    return jsonify({'companies': companies})

@app.route('/api/tracked-companies', methods=['POST'])
def add_tracked_company():
    """Add a new tracked company."""
    data = request.json
    company_name = data.get('company_name', '').strip()
    career_page_url = data.get('career_page_url', '').strip()
    job_alert_email = data.get('job_alert_email', '').strip()
    notes = data.get('notes', '').strip()

    if not company_name:
        return jsonify({'error': 'Company name is required'}), 400

    conn = get_db()
    now = datetime.now().isoformat()
    conn.execute(
        """INSERT INTO tracked_companies
           (company_name, career_page_url, job_alert_email, notes, created_at, updated_at)
           VALUES (?, ?, ?, ?, ?, ?)""",
        (company_name, career_page_url, job_alert_email, notes, now, now)
    )
    conn.commit()
    conn.close()
    return jsonify({'success': True})

@app.route('/api/tracked-companies/<int:company_id>', methods=['PATCH'])
def update_tracked_company(company_id):
    """Update a tracked company."""
    data = request.json
    company_name = data.get('company_name', '').strip()
    career_page_url = data.get('career_page_url', '').strip()
    job_alert_email = data.get('job_alert_email', '').strip()
    notes = data.get('notes', '').strip()

    if not company_name:
        return jsonify({'error': 'Company name is required'}), 400

    conn = get_db()
    now = datetime.now().isoformat()
    conn.execute(
        """UPDATE tracked_companies
           SET company_name = ?, career_page_url = ?, job_alert_email = ?, notes = ?, updated_at = ?
           WHERE id = ?""",
        (company_name, career_page_url, job_alert_email, notes, now, company_id)
    )
    conn.commit()
    conn.close()
    return jsonify({'success': True})

@app.route('/api/tracked-companies/<int:company_id>', methods=['DELETE'])
def delete_tracked_company(company_id):
    """Delete a tracked company."""
    conn = get_db()
    conn.execute("DELETE FROM tracked_companies WHERE id = ?", (company_id,))
    conn.commit()
    conn.close()
    return jsonify({'success': True})

# ============== Custom Email Sources ==============
@app.route('/api/custom-email-sources', methods=['GET'])
def get_custom_email_sources():
    """Get all custom email sources."""
    conn = get_db()
    sources = [dict(row) for row in conn.execute(
        "SELECT * FROM custom_email_sources ORDER BY created_at DESC"
    ).fetchall()]
    conn.close()
    return jsonify({'sources': sources})

@app.route('/api/custom-email-sources', methods=['POST'])
def add_custom_email_source():
    """Add a new custom email source."""
    data = request.json
    name = data.get('name', '').strip()
    sender_email = data.get('sender_email', '').strip()
    sender_pattern = data.get('sender_pattern', '').strip()
    subject_keywords = data.get('subject_keywords', '').strip()

    if not name:
        return jsonify({'error': 'Source name is required'}), 400

    if not sender_email and not sender_pattern:
        return jsonify({'error': 'Either sender email or pattern is required'}), 400

    conn = get_db()
    now = datetime.now().isoformat()
    conn.execute(
        """INSERT INTO custom_email_sources
           (name, sender_email, sender_pattern, subject_keywords, enabled, created_at, updated_at)
           VALUES (?, ?, ?, ?, 1, ?, ?)""",
        (name, sender_email, sender_pattern, subject_keywords, now, now)
    )
    conn.commit()
    conn.close()
    return jsonify({'success': True})

@app.route('/api/custom-email-sources/<int:source_id>', methods=['PATCH'])
def update_custom_email_source(source_id):
    """Update a custom email source."""
    data = request.json
    name = data.get('name', '').strip()
    sender_email = data.get('sender_email', '').strip()
    sender_pattern = data.get('sender_pattern', '').strip()
    subject_keywords = data.get('subject_keywords', '').strip()
    enabled = data.get('enabled', 1)

    if not name:
        return jsonify({'error': 'Source name is required'}), 400

    conn = get_db()
    now = datetime.now().isoformat()
    conn.execute(
        """UPDATE custom_email_sources
           SET name = ?, sender_email = ?, sender_pattern = ?, subject_keywords = ?, enabled = ?, updated_at = ?
           WHERE id = ?""",
        (name, sender_email, sender_pattern, subject_keywords, enabled, now, source_id)
    )
    conn.commit()
    conn.close()
    return jsonify({'success': True})

@app.route('/api/custom-email-sources/<int:source_id>', methods=['DELETE'])
def delete_custom_email_source(source_id):
    """Delete a custom email source."""
    conn = get_db()
    conn.execute("DELETE FROM custom_email_sources WHERE id = ?", (source_id,))
    conn.commit()
    conn.close()
    return jsonify({'success': True})

# ============== External Applications ==============
@app.route('/api/external-applications', methods=['GET'])
def get_external_applications():
    """
    Get all external applications with optional filtering.
    Query params: status, company, source
    """
    print(f"[Backend] GET /api/external-applications - Query params: {dict(request.args)}")
    conn = get_db()

    # Build query with optional filters
    query = "SELECT * FROM external_applications WHERE 1=1"
    params = []

    if request.args.get('status'):
        query += " AND status = ?"
        params.append(request.args.get('status'))

    if request.args.get('company'):
        query += " AND company LIKE ?"
        params.append(f"%{request.args.get('company')}%")

    if request.args.get('source'):
        query += " AND source = ?"
        params.append(request.args.get('source'))

    query += " ORDER BY applied_date DESC"

    applications = [dict(row) for row in conn.execute(query, params).fetchall()]
    conn.close()

    print(f"[Backend] Returning {len(applications)} external applications")
    return jsonify({'applications': applications})

@app.route('/api/external-applications', methods=['POST'])
def create_external_application():
    """
    Create a new external application.
    Required fields: title, company, applied_date, source

    This endpoint now creates BOTH:
    1. An entry in the external_applications table (for tracking application details)
    2. An entry in the jobs table (so it appears in the main job list with status='applied')
    """
    data = request.json
    print(f"[Backend] POST /api/external-applications - Received data: {data}")

    # Validate required fields
    required_fields = ['title', 'company', 'applied_date', 'source']
    for field in required_fields:
        if not data.get(field):
            print(f"[Backend] Validation failed: {field} is missing")
            return jsonify({'error': f'{field} is required'}), 400

    # Generate unique IDs
    app_id = str(uuid.uuid4())[:16]
    job_id = str(uuid.uuid4())[:16]  # Create a new job_id for the job entry
    print(f"[Backend] Generated app_id: {app_id}, job_id: {job_id}")

    # Get optional fields
    location = data.get('location', '')
    url = data.get('url', '')
    application_method = data.get('application_method', '')
    contact_name = data.get('contact_name', '')
    contact_email = data.get('contact_email', '')
    status = data.get('status', 'applied')
    follow_up_date = data.get('follow_up_date')
    notes = data.get('notes', '')

    now = datetime.now().isoformat()

    try:
        conn = get_db()

        # First, create a job entry in the jobs table
        # This makes the external application show up in the main job list
        conn.execute('''
            INSERT INTO jobs (
                job_id, title, company, location, url, source,
                status, score, baseline_score, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            job_id, data['title'], data['company'], location, url,
            f"external_{data['source']}", # Prefix source to identify it came from external tracking
            'applied',  # Set status to 'applied' since this is an external application
            0,  # Default score
            0,  # Default baseline score
            now, now
        ))
        print(f"[Backend] Created job entry with job_id: {job_id}")

        # Then, create the external application entry linked to the job
        conn.execute('''
            INSERT INTO external_applications (
                app_id, job_id, title, company, location, url, source,
                application_method, applied_date, contact_name, contact_email,
                status, follow_up_date, notes, created_at, updated_at, is_linked_to_job
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            app_id, job_id, data['title'], data['company'], location, url, data['source'],
            application_method, data['applied_date'], contact_name, contact_email,
            status, follow_up_date, notes, now, now, 1  # is_linked_to_job = 1
        ))
        print(f"[Backend] Created external application entry linked to job_id: {job_id}")

        conn.commit()
        conn.close()
        print(f"[Backend] Successfully inserted external application and job: {data['title']} at {data['company']}")
    except Exception as e:
        print(f"[Backend] Database error: {str(e)}")
        return jsonify({'error': f'Database error: {str(e)}'}), 500

    return jsonify({'success': True, 'app_id': app_id, 'job_id': job_id})

@app.route('/api/external-applications/<app_id>', methods=['GET'])
def get_external_application(app_id):
    """Get details of a specific external application."""
    conn = get_db()
    app = conn.execute(
        "SELECT * FROM external_applications WHERE app_id = ?",
        (app_id,)
    ).fetchone()
    conn.close()

    if not app:
        return jsonify({'error': 'Application not found'}), 404

    return jsonify({'application': dict(app)})

@app.route('/api/external-applications/<app_id>', methods=['PATCH'])
def update_external_application(app_id):
    """Update an external application and sync status to linked job."""
    data = request.json
    conn = get_db()

    # Build update query dynamically
    allowed_fields = [
        'title', 'company', 'location', 'url', 'source', 'application_method',
        'applied_date', 'contact_name', 'contact_email', 'status',
        'follow_up_date', 'notes', 'job_id', 'is_linked_to_job'
    ]
    updates = []
    params = []

    for field in allowed_fields:
        if field in data:
            updates.append(f"{field} = ?")
            params.append(data[field])

    if updates:
        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(app_id)

        query = f"UPDATE external_applications SET {', '.join(updates)} WHERE app_id = ?"
        conn.execute(query, params)

        # If status is being updated, also update the linked job's status
        if 'status' in data:
            # Get the job_id for this external application
            app = conn.execute(
                "SELECT job_id FROM external_applications WHERE app_id = ?",
                (app_id,)
            ).fetchone()

            if app and app['job_id']:
                conn.execute(
                    "UPDATE jobs SET status = ?, updated_at = ? WHERE job_id = ?",
                    (data['status'], datetime.now().isoformat(), app['job_id'])
                )
                print(f"[Backend] Synced status '{data['status']}' to linked job: {app['job_id']}")

        conn.commit()

    conn.close()
    return jsonify({'success': True})

@app.route('/api/external-applications/<app_id>', methods=['DELETE'])
def delete_external_application(app_id):
    """Delete an external application and its linked job."""
    conn = get_db()

    # First, get the job_id if it exists
    app = conn.execute(
        "SELECT job_id FROM external_applications WHERE app_id = ?",
        (app_id,)
    ).fetchone()

    # Delete the external application
    conn.execute("DELETE FROM external_applications WHERE app_id = ?", (app_id,))

    # If there's a linked job, delete it too
    if app and app['job_id']:
        conn.execute("DELETE FROM jobs WHERE job_id = ?", (app['job_id'],))
        print(f"[Backend] Deleted linked job: {app['job_id']}")

    conn.commit()
    conn.close()
    return jsonify({'success': True})

# ============== Resume Management ==============
@app.route('/api/resumes', methods=['GET'])
def get_resumes():
    """Get all resume variants."""
    print("[Backend] GET /api/resumes")
    resumes = load_resumes_from_db()
    print(f"[Backend] Returning {len(resumes)} resumes")
    return jsonify({'resumes': resumes})

@app.route('/api/resumes/<resume_id>', methods=['GET'])
def get_resume(resume_id):
    """Get a specific resume by ID."""
    print(f"[Backend] GET /api/resumes/{resume_id}")
    conn = get_db()
    resume = conn.execute(
        "SELECT * FROM resume_variants WHERE resume_id = ?",
        (resume_id,)
    ).fetchone()
    conn.close()

    if not resume:
        return jsonify({'error': 'Resume not found'}), 404

    return jsonify({'resume': dict(resume)})

@app.route('/api/resumes', methods=['POST'])
def create_resume():
    """
    Create a new resume variant.
    Accepts either file upload or direct content.
    """
    print("[Backend] POST /api/resumes")
    data = request.json

    # Validate required fields
    if not data.get('name'):
        return jsonify({'error': 'name is required'}), 400

    if not data.get('content'):
        return jsonify({'error': 'content is required'}), 400

    # Generate ID and hash
    resume_id = str(uuid.uuid4())[:16]
    content = data['content']
    content_hash = hashlib.sha256(content.encode()).hexdigest()
    now = datetime.now().isoformat()

    # Get optional fields
    focus_areas = data.get('focus_areas', '')
    target_roles = data.get('target_roles', '')
    file_path = data.get('file_path', '')

    try:
        conn = get_db()

        # Check for duplicate content
        existing = conn.execute(
            "SELECT resume_id, name FROM resume_variants WHERE content_hash = ?",
            (content_hash,)
        ).fetchone()

        if existing:
            conn.close()
            return jsonify({
                'error': f'This resume already exists as "{existing["name"]}"',
                'existing_id': existing['resume_id']
            }), 409

        # Insert new resume
        conn.execute('''
            INSERT INTO resume_variants (
                resume_id, name, focus_areas, target_roles, file_path,
                content, content_hash, created_at, updated_at, is_active
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 1)
        ''', (resume_id, data['name'], focus_areas, target_roles, file_path,
              content, content_hash, now, now))

        conn.commit()
        conn.close()

        print(f"[Backend] Created resume: {data['name']}")
        return jsonify({'success': True, 'resume_id': resume_id})

    except Exception as e:
        print(f"[Backend] Error creating resume: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/resumes/upload', methods=['POST'])
def upload_resume():
    """
    Upload a resume file (PDF, TXT, or MD) and extract text.
    """
    print("[Backend] POST /api/resumes/upload")

    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400

    file = request.files['file']

    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400

    # Validate file extension
    allowed_extensions = {'.pdf', '.txt', '.md'}
    filename = secure_filename(file.filename)
    file_ext = os.path.splitext(filename)[1].lower()

    if file_ext not in allowed_extensions:
        return jsonify({'error': f'Invalid file type. Only PDF, TXT, MD allowed'}), 400

    try:
        # Extract text based on file type
        if file_ext == '.pdf':
            # Try to extract PDF text
            try:
                from pypdf import PdfReader
                pdf_reader = PdfReader(file)
                text_parts = []
                for page in pdf_reader.pages:
                    text_parts.append(page.extract_text())
                resume_text = '\n\n'.join(text_parts)

                if not resume_text.strip():
                    return jsonify({'error': 'Could not extract text from PDF. It may be scanned or image-based.'}), 400

            except ImportError:
                return jsonify({
                    'error': 'PDF support not installed. Install with: pip install pypdf',
                    'hint': 'For now, please copy text from PDF and use "Paste Text" mode'
                }), 400
            except Exception as e:
                return jsonify({'error': f'PDF extraction failed: {str(e)}'}), 400
        else:
            # Text or Markdown file
            resume_text = file.read().decode('utf-8')

        # Get metadata from form
        name = request.form.get('name', filename.rsplit('.', 1)[0])
        focus_areas = request.form.get('focus_areas', '')
        target_roles = request.form.get('target_roles', '')

        # Generate ID and hash
        resume_id = str(uuid.uuid4())[:16]
        content_hash = hashlib.sha256(resume_text.encode()).hexdigest()
        now = datetime.now().isoformat()

        conn = get_db()

        # Check for duplicate content
        existing = conn.execute(
            "SELECT resume_id, name FROM resume_variants WHERE content_hash = ?",
            (content_hash,)
        ).fetchone()

        if existing:
            conn.close()
            return jsonify({
                'error': f'This resume already exists as "{existing["name"]}"',
                'existing_id': existing['resume_id']
            }), 409

        # Insert new resume
        conn.execute('''
            INSERT INTO resume_variants (
                resume_id, name, focus_areas, target_roles, file_path,
                content, content_hash, created_at, updated_at, is_active
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 1)
        ''', (resume_id, name, focus_areas, target_roles, filename,
              resume_text, content_hash, now, now))

        conn.commit()
        conn.close()

        print(f"[Backend] Uploaded resume: {name} ({len(resume_text)} chars)")
        return jsonify({
            'success': True,
            'resume_id': resume_id,
            'name': name,
            'text_length': len(resume_text),
            'pages_extracted': resume_text.count('\n\n') + 1 if file_ext == '.pdf' else 1
        })

    except Exception as e:
        print(f"[Backend] Error uploading resume: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/resumes/<resume_id>', methods=['PATCH'])
def update_resume(resume_id):
    """Update resume metadata (not content - that requires new variant)."""
    print(f"[Backend] PATCH /api/resumes/{resume_id}")
    data = request.json
    conn = get_db()

    # Build update query for metadata only
    allowed_fields = ['name', 'focus_areas', 'target_roles', 'is_active']
    updates = []
    params = []

    for field in allowed_fields:
        if field in data:
            updates.append(f"{field} = ?")
            params.append(data[field])

    if updates:
        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(resume_id)

        query = f"UPDATE resume_variants SET {', '.join(updates)} WHERE resume_id = ?"
        conn.execute(query, params)
        conn.commit()

    conn.close()
    print(f"[Backend] Updated resume: {resume_id}")
    return jsonify({'success': True})

@app.route('/api/resumes/<resume_id>', methods=['DELETE'])
def delete_resume(resume_id):
    """Soft delete a resume (sets is_active = 0)."""
    print(f"[Backend] DELETE /api/resumes/{resume_id}")
    conn = get_db()

    conn.execute(
        "UPDATE resume_variants SET is_active = 0, updated_at = ? WHERE resume_id = ?",
        (datetime.now().isoformat(), resume_id)
    )

    conn.commit()
    conn.close()

    print(f"[Backend] Deactivated resume: {resume_id}")
    return jsonify({'success': True})

# ============== Resume Recommendations ==============
@app.route('/api/jobs/<job_id>/recommend-resume', methods=['POST'])
def get_resume_recommendation(job_id):
    """Get AI resume recommendation for a specific job."""
    print(f"[Backend] POST /api/jobs/{job_id}/recommend-resume")

    conn = get_db()
    job = conn.execute(
        "SELECT * FROM jobs WHERE job_id = ?",
        (job_id,)
    ).fetchone()

    if not job:
        conn.close()
        return jsonify({'error': 'Job not found'}), 404

    job_dict = dict(job)

    # Check if recommendation already exists
    if job_dict.get('resume_recommendation'):
        try:
            cached_rec = json.loads(job_dict['resume_recommendation'])
            conn.close()
            print(f"[Backend] Returning cached recommendation for {job_id}")
            return jsonify({'recommendation': cached_rec, 'cached': True})
        except:
            pass  # If parsing fails, generate new recommendation

    # Generate new recommendation
    try:
        recommendation = recommend_resume_for_job(
            job_dict.get('raw_text', ''),
            job_dict.get('title', ''),
            job_dict.get('company', '')
        )

        # Store recommendation in database
        now = datetime.now().isoformat()
        conn.execute("""
            UPDATE jobs
            SET recommended_resume_id = ?,
                resume_recommendation = ?,
                resume_match_score = ?,
                updated_at = ?
            WHERE job_id = ?
        """, (
            recommendation['resume_id'],
            json.dumps(recommendation),
            recommendation['confidence'],
            now,
            job_id
        ))

        # Log the recommendation
        log_id = str(uuid.uuid4())[:16]
        conn.execute("""
            INSERT INTO resume_usage_log (
                log_id, resume_id, job_id, recommended_at,
                confidence_score, reasoning
            ) VALUES (?, ?, ?, ?, ?, ?)
        """, (
            log_id,
            recommendation['resume_id'],
            job_id,
            now,
            recommendation['confidence'],
            recommendation['reasoning']
        ))

        # Update resume usage count
        conn.execute("""
            UPDATE resume_variants
            SET usage_count = usage_count + 1
            WHERE resume_id = ?
        """, (recommendation['resume_id'],))

        conn.commit()
        conn.close()

        print(f"[Backend] Generated resume recommendation for {job_id}: {recommendation['resume_name']}")
        return jsonify({'recommendation': recommendation, 'cached': False})

    except Exception as e:
        conn.close()
        print(f"[Backend] Error generating recommendation: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/jobs/recommend-resumes-batch', methods=['POST'])
def batch_recommend_resumes():
    """
    Generate resume recommendations for multiple jobs in batch.
    Processes up to 100 jobs with rate limiting.
    """
    print("[Backend] POST /api/jobs/recommend-resumes-batch")
    data = request.json
    job_ids = data.get('job_ids', [])

    if not job_ids:
        return jsonify({'error': 'job_ids required'}), 400

    if len(job_ids) > 100:
        return jsonify({'error': 'Maximum 100 jobs per batch'}), 400

    conn = get_db()
    results = []
    errors = []

    for idx, job_id in enumerate(job_ids):
        try:
            # Get job
            job = conn.execute(
                "SELECT * FROM jobs WHERE job_id = ?",
                (job_id,)
            ).fetchone()

            if not job:
                errors.append({'job_id': job_id, 'error': 'Job not found'})
                continue

            job_dict = dict(job)

            # Skip if already has recommendation
            if job_dict.get('resume_recommendation'):
                results.append({
                    'job_id': job_id,
                    'status': 'skipped',
                    'reason': 'Already has recommendation'
                })
                continue

            # Generate recommendation
            recommendation = recommend_resume_for_job(
                job_dict.get('raw_text', ''),
                job_dict.get('title', ''),
                job_dict.get('company', '')
            )

            # Store in database
            now = datetime.now().isoformat()
            conn.execute("""
                UPDATE jobs
                SET recommended_resume_id = ?,
                    resume_recommendation = ?,
                    resume_match_score = ?,
                    updated_at = ?
                WHERE job_id = ?
            """, (
                recommendation['resume_id'],
                json.dumps(recommendation),
                recommendation['confidence'],
                now,
                job_id
            ))

            # Log the recommendation
            log_id = str(uuid.uuid4())[:16]
            conn.execute("""
                INSERT INTO resume_usage_log (
                    log_id, resume_id, job_id, recommended_at,
                    confidence_score, reasoning
                ) VALUES (?, ?, ?, ?, ?, ?)
            """, (
                log_id,
                recommendation['resume_id'],
                job_id,
                now,
                recommendation['confidence'],
                recommendation['reasoning']
            ))

            # Update usage count
            conn.execute("""
                UPDATE resume_variants
                SET usage_count = usage_count + 1
                WHERE resume_id = ?
            """, (recommendation['resume_id'],))

            results.append({
                'job_id': job_id,
                'status': 'success',
                'resume_id': recommendation['resume_id'],
                'resume_name': recommendation['resume_name'],
                'confidence': recommendation['confidence']
            })

            # Rate limiting: small delay between API calls
            if idx < len(job_ids) - 1:  # Don't delay after last one
                import time
                time.sleep(0.5)

        except Exception as e:
            errors.append({'job_id': job_id, 'error': str(e)})
            print(f"[Backend] Error processing {job_id}: {e}")

    conn.commit()
    conn.close()

    success_count = len([r for r in results if r['status'] == 'success'])
    print(f"[Backend] Batch recommendation complete: {success_count}/{len(job_ids)} successful")

    return jsonify({
        'success': True,
        'results': results,
        'errors': errors,
        'summary': {
            'total': len(job_ids),
            'successful': success_count,
            'skipped': len([r for r in results if r['status'] == 'skipped']),
            'failed': len(errors)
        }
    })

@app.route('/api/research-jobs', methods=['POST'])
def research_jobs():
    """
    Use Claude AI to research and recommend jobs based on user's resume and location preferences.
    Generates 5-10 job recommendations with company names, roles, and why they're a good fit.
    """
    print("[Backend] POST /api/research-jobs - Starting Claude job research")

    try:
        # Load resumes from database
        resumes = load_resumes_from_db()
        if not resumes:
            return jsonify({'error': 'No resumes found in database'}), 400

        # Combine resume content
        resume_text = "\n\n---RESUME VARIANT---\n\n".join([
            f"{r['name']}\nFocus: {r.get('focus_areas', 'N/A')}\n\n{r['content']}"
            for r in resumes
        ])

        # Get location preferences from config
        primary_locations = [loc['name'] for loc in CONFIG.primary_locations]

        # Get experience level and preferences
        exp_level_dict = CONFIG.experience_level
        exp_level = exp_level_dict.get('current_level', 'mid')
        min_years = exp_level_dict.get('min_years', 1)
        max_years = exp_level_dict.get('max_years', 5)

        # Create research prompt
        research_prompt = f"""You are a job search research assistant. Based on the candidate's resume and preferences, recommend 5-10 specific job opportunities they should pursue.

CANDIDATE'S RESUME:
{resume_text[:10000]}

LOCATION PREFERENCES:
Primary locations: {', '.join(primary_locations)}

EXPERIENCE LEVEL:
Current level: {exp_level}
Years of experience: {min_years}-{max_years}

TASK:
Research and recommend 5-10 specific job opportunities that:
1. Match the candidate's skills and experience level
2. Are in their preferred locations (especially remote opportunities)
3. Are realistic and currently in-demand roles
4. Align with their career trajectory

For each job recommendation, provide:
- Company name (real companies that commonly hire for these roles)
- Job title
- Career page URL (if you know the company's career/jobs page URL, otherwise leave blank)
- Why it's a good fit (2-3 specific reasons based on their resume)
- Key skills from their resume that match
- Estimated match score (0-100)

Return ONLY a valid JSON array with this structure:
[
  {{
    "company": "Company Name",
    "title": "Job Title",
    "location": "Location",
    "career_page_url": "https://company.com/careers or blank if unknown",
    "why_good_fit": "Specific reasons why this role matches their background...",
    "matching_skills": ["skill1", "skill2", "skill3"],
    "match_score": 85,
    "job_type": "Full-time"
  }}
]

Focus on real, reputable companies and current in-demand roles. Be specific and actionable."""

        # Call Claude API
        client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

        response = client.messages.create(
            model=CONFIG.ai_model or "claude-sonnet-4-20250514",
            max_tokens=4000,
            temperature=0.7,
            messages=[{
                "role": "user",
                "content": research_prompt
            }]
        )

        # Parse response
        response_text = response.content[0].text.strip()

        # Extract JSON from response
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0].strip()
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0].strip()

        recommendations = json.loads(response_text)

        print(f"[Backend] Claude generated {len(recommendations)} job recommendations")

        # Save recommendations to database as "researched" jobs
        conn = get_db()
        saved_jobs = []
        now = datetime.now().isoformat()

        for rec in recommendations[:10]:  # Limit to 10
            # Generate job ID
            job_id = hashlib.sha256(
                f"{rec['company']}:{rec['title']}:claude_research".encode()
            ).hexdigest()[:16]

            # Check if already exists
            existing = conn.execute(
                "SELECT job_id FROM jobs WHERE job_id = ?",
                (job_id,)
            ).fetchone()

            if existing:
                print(f"[Backend] Skipping duplicate: {rec['title']} at {rec['company']}")
                continue

            # Create analysis JSON
            analysis = {
                'qualification_score': rec.get('match_score', 80),
                'should_apply': True,
                'strengths': rec.get('matching_skills', []),
                'gaps': [],
                'recommendation': rec.get('why_good_fit', ''),
                'resume_to_use': resumes[0]['name'] if resumes else 'default'
            }

            # Generate job URL - prefer career page, fallback to Google Jobs search
            career_url = rec.get('career_page_url', '').strip()
            if career_url and career_url.startswith('http'):
                job_url = career_url
            else:
                # Create Google Jobs search URL (better than regular Google search)
                search_query = f"{rec['title']} {rec['company']}".replace(' ', '+')
                job_url = f"https://www.google.com/search?q={search_query}&ibp=htl;jobs"

            # Insert into database
            conn.execute('''
                INSERT INTO jobs (
                    job_id, title, company, location, url, source,
                    status, score, baseline_score, analysis, raw_text,
                    created_at, updated_at, is_filtered
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                job_id,
                rec['title'],
                rec['company'],
                rec.get('location', primary_locations[0] if primary_locations else 'Remote'),
                job_url,
                'claude_research',
                'new',
                rec.get('match_score', 80),
                rec.get('match_score', 80),
                json.dumps(analysis),
                rec.get('why_good_fit', ''),
                now,
                now,
                0
            ))

            saved_jobs.append({
                'job_id': job_id,
                'title': rec['title'],
                'company': rec['company'],
                'score': rec.get('match_score', 80)
            })

        conn.commit()
        conn.close()

        print(f"[Backend] Saved {len(saved_jobs)} new research jobs to database")

        return jsonify({
            'success': True,
            'jobs_found': len(recommendations),
            'jobs_saved': len(saved_jobs),
            'saved_jobs': saved_jobs
        })

    except Exception as e:
        print(f"[Backend] Error in job research: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Research failed: {str(e)}'}), 500

@app.route('/api/research-jobs/<resume_id>', methods=['POST'])
def research_jobs_for_resume(resume_id):
    """
    Research jobs tailored specifically to a single resume.
    Uses Claude AI to find 5-10 jobs that match this resume's focus areas and target roles.
    """
    print(f"[Backend] POST /api/research-jobs/{resume_id}")

    try:
        conn = get_db()

        # Get the resume
        resume = conn.execute(
            "SELECT * FROM resume_variants WHERE resume_id = ? AND is_active = 1",
            (resume_id,)
        ).fetchone()

        if not resume:
            return jsonify({'error': 'Resume not found'}), 404

        resume_name = resume['name']
        resume_content = resume['content']
        focus_areas = resume['focus_areas'] or 'Not specified'
        target_roles = resume['target_roles'] or 'Not specified'

        print(f"[Backend] Researching jobs for resume: {resume_name}")
        print(f"  Focus areas: {focus_areas}")
        print(f"  Target roles: {target_roles}")

        # Build research prompt tailored to this specific resume
        research_prompt = f"""You are a job search assistant. Research and recommend specific job opportunities for a candidate based on their resume.

CANDIDATE'S RESUME:
{resume_content[:3000]}

FOCUS AREAS: {focus_areas}
TARGET ROLES: {target_roles}

LOCATION PREFERENCES:
Primary locations: {', '.join([loc.get('name', '') for loc in CONFIG.preferences.get('locations', {}).get('primary', [])])}

TASK:
Research and recommend 5-10 specific job opportunities that:
1. Match this resume's specific skills and experience
2. Align with the focus areas: {focus_areas}
3. Match target roles: {target_roles}
4. Are in preferred locations (especially remote opportunities)
5. Are realistic and currently in-demand roles

For each job recommendation, provide:
- Company name (real companies that commonly hire for these roles)
- Job title
- Career page URL (if you know the company's career/jobs page URL, otherwise leave blank)
- Why it's a perfect fit for THIS specific resume (2-3 reasons citing actual resume content)
- Key skills from this resume that match
- Estimated match score (0-100)

Return ONLY a valid JSON array with this structure:
[
  {{
    "company": "Company Name",
    "title": "Job Title",
    "location": "Location",
    "career_page_url": "https://company.com/careers or blank if unknown",
    "why_good_fit": "Specific reasons why this role matches THIS resume...",
    "matching_skills": ["skill1", "skill2", "skill3"],
    "match_score": 85,
    "job_type": "Full-time"
  }}
]

Focus on roles that specifically match the focus areas and target roles for THIS resume."""

        # Call Claude API
        client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

        response = client.messages.create(
            model=CONFIG.ai_model or "claude-sonnet-4-20250514",
            max_tokens=4000,
            temperature=0.7,
            messages=[{
                "role": "user",
                "content": research_prompt
            }]
        )

        # Parse response
        response_text = response.content[0].text.strip()

        # Extract JSON from response
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0].strip()
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0].strip()

        recommendations = json.loads(response_text)

        print(f"[Backend] Claude generated {len(recommendations)} job recommendations for {resume_name}")

        # Save recommendations to database
        saved_jobs = []
        now = datetime.now().isoformat()

        for rec in recommendations[:10]:  # Limit to 10
            # Generate job ID
            job_id = hashlib.sha256(
                f"{rec['company']}:{rec['title']}:resume_{resume_id}".encode()
            ).hexdigest()[:16]

            # Check if already exists
            existing = conn.execute(
                "SELECT job_id FROM jobs WHERE job_id = ?",
                (job_id,)
            ).fetchone()

            if existing:
                print(f"[Backend] Skipping duplicate: {rec['title']} at {rec['company']}")
                continue

            # Create analysis JSON
            analysis = {
                'qualification_score': rec.get('match_score', 80),
                'should_apply': True,
                'strengths': rec.get('matching_skills', []),
                'gaps': [],
                'recommendation': rec.get('why_good_fit', ''),
                'resume_to_use': resume_name
            }

            # Generate job URL - prefer career page, fallback to Google Jobs search
            career_url = rec.get('career_page_url', '').strip()
            if career_url and career_url.startswith('http'):
                job_url = career_url
            else:
                # Create Google Jobs search URL (better than regular Google search)
                search_query = f"{rec['title']} {rec['company']}".replace(' ', '+')
                job_url = f"https://www.google.com/search?q={search_query}&ibp=htl;jobs"

            # Insert into database
            conn.execute('''
                INSERT INTO jobs (
                    job_id, title, company, location, url, source,
                    status, score, baseline_score, analysis, raw_text,
                    created_at, updated_at, is_filtered
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0)
            ''', (
                job_id,
                rec['title'],
                rec['company'],
                rec.get('location', 'Remote'),
                job_url,
                f'claude_research_{resume_name}',
                'new',
                rec.get('match_score', 80),
                rec.get('match_score', 80),
                json.dumps(analysis),
                json.dumps(rec),
                now,
                now
            ))

            saved_jobs.append({
                'title': rec['title'],
                'company': rec['company'],
                'score': rec.get('match_score', 80)
            })

        conn.commit()
        conn.close()

        return jsonify({
            'success': True,
            'jobs_found': len(recommendations),
            'jobs_saved': len(saved_jobs),
            'resume_name': resume_name,
            'saved_jobs': saved_jobs
        })

    except Exception as e:
        print(f"[Backend] Error in resume-specific job research: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Research failed: {str(e)}'}), 500

# ============== Backup API Routes ==============

@app.route('/api/backup/create', methods=['POST'])
def api_create_backup():
    """Create a manual database backup."""
    try:
        backup_manager = BackupManager(DB_PATH, max_backups=10)
        backup_path = backup_manager.create_backup()

        if backup_path:
            return jsonify({
                'success': True,
                'message': 'Backup created successfully',
                'filename': backup_path.name,
                'path': str(backup_path)
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Failed to create backup'
            }), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/backup/list', methods=['GET'])
def api_list_backups():
    """List all available backups."""
    try:
        backup_manager = BackupManager(DB_PATH)
        backups = backup_manager.list_backups()
        stats = backup_manager.get_backup_stats()

        return jsonify({
            'backups': backups,
            'stats': stats
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/backup/restore/<filename>', methods=['POST'])
def api_restore_backup(filename):
    """Restore database from a backup file."""
    try:
        backup_manager = BackupManager(DB_PATH)
        success = backup_manager.restore_backup(filename)

        if success:
            return jsonify({
                'success': True,
                'message': f'Database restored from {filename}'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Restore failed'
            }), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    # Initialize database
    init_db()
    RESUMES_DIR.mkdir(exist_ok=True)

    # Create automatic backup on startup
    print("ðŸ”„ Creating automatic backup...")
    if backup_on_startup(DB_PATH, max_backups=10):
        print("âœ… Backup created successfully")
    else:
        print("âš ï¸  Backup skipped (database may not exist yet)")

    # Migrate existing resumes from files to database
    try:
        migrate_file_resumes_to_db()
    except Exception as e:
        print(f"âš ï¸  Resume migration skipped: {e}")

    # Display startup info
    print("\n" + "="*60)
    print("ðŸ· Hammy the Hire Tracker - Go HAM on Your Job Search!")
    print("="*60)
    print(f"\nðŸ‘¤ User: {CONFIG.user_name}")
    print(f"ðŸ“§ Email: {CONFIG.user_email}")
    print(f"ðŸ“„ Resumes loaded: {len(CONFIG.resume_files)}")
    print(f"\nðŸ“ Configuration: {APP_DIR / 'config.yaml'}")
    print(f"ðŸ“ Database: {DB_PATH}")
    print(f"ðŸ“ Gmail credentials: {CREDENTIALS_FILE}")
    print(f"\nðŸ’¡ Hammy's Quick Start Guide:")
    print(f"   1. Click 'ðŸ“§ Scan Gmail' to import job alerts")
    print(f"   2. Click 'ðŸ¤– Analyze All' for AI analysis")
    print(f"   3. Click 'ðŸ“¬ Scan Follow-Ups' to track responses")
    print(f"   4. Use the Chrome extension for instant analysis")
    print(f"\nðŸš€ Dashboard running at: http://localhost:5000")
    print("="*60 + "\n")

    # Start Flask app
    app.run(debug=True, port=5000)